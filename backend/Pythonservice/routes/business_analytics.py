"""
Business Analytics API Route
Endpoint ƒë·ªÉ ph√¢n t√≠ch d·ªØ li·ªáu kinh doanh v√† ƒë·ªÅ xu·∫•t chi·∫øn l∆∞·ª£c b·∫±ng AI
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
import google.generativeai as genai
import os
from datetime import datetime, timedelta
import json
from typing import Optional, Dict, Any, List
import chromadb
from groq import Groq
import requests
import base64

# Import services
from services.document_processing_service import get_document_processor
from services.analytics_rag_service import AnalyticsRAGService
from services.forecasting_service import get_forecasting_service

router = APIRouter()

# Global analytics RAG service instance
analytics_rag_service = None

def set_analytics_rag_service(service: AnalyticsRAGService):
    """Set the global analytics RAG service instance"""
    global analytics_rag_service
    analytics_rag_service = service

# Helper functions for safe type conversion
def safe_decimal(value):
    """Safely convert value to float"""
    if value is None:
        return 0.0
    try:
        return float(value)
    except (ValueError, TypeError):
        return 0.0

def safe_int(value):
    """Safely convert value to int"""
    if value is None:
        return 0
    try:
        return int(value)
    except (ValueError, TypeError):
        return 0

def safe_str(value):
    """Safely convert value to string"""
    if value is None:
        return ""
    return str(value)

def sanitize_metadata(metadata_dict):
    """
    Sanitize metadata dictionary for ChromaDB compatibility
    Enhanced version with better validation and type handling
    """
    sanitized = {}
    for key, value in metadata_dict.items():
        # Skip None values
        if value is None:
            continue
            
        # Handle different data types
        if isinstance(value, bool):
            sanitized[key] = value
        elif isinstance(value, (int, float)):
            # Ensure numeric values are valid
            if not (value != value):  # Check for NaN
                # Limit numeric range to prevent overflow
                if isinstance(value, float):
                    sanitized[key] = max(-1e10, min(1e10, value))
                else:
                    sanitized[key] = max(-2147483648, min(2147483647, value))
        elif isinstance(value, str):
            # Clean and truncate strings
            cleaned = value.replace('\x00', '').replace('\r', ' ').replace('\n', ' ')
            # Remove excessive whitespace
            cleaned = ' '.join(cleaned.split())
            # Limit string length (ChromaDB metadata limit)
            if len(cleaned) > 5000:  # Reduced from 10000 for safety
                cleaned = cleaned[:4997] + '...'
            sanitized[key] = cleaned
        elif isinstance(value, (list, tuple)):
            # Convert lists to comma-separated string
            str_list = [str(item) for item in value if item is not None]
            sanitized[key] = ', '.join(str_list)[:5000]
        elif isinstance(value, dict):
            # Convert dict to JSON string (limited length)
            try:
                import json
                json_str = json.dumps(value, ensure_ascii=False)
                if len(json_str) > 5000:
                    json_str = json_str[:4997] + '...'
                sanitized[key] = json_str
            except:
                sanitized[key] = str(value)[:5000]
        else:
            # Fallback: convert to string
            sanitized[key] = str(value)[:5000]
    
    return sanitized

def parse_jwt_token(token: str) -> Optional[Dict[str, Any]]:
    """
    Parse JWT token ƒë·ªÉ l·∫•y payload (kh√¥ng verify signature - server s·∫Ω verify)
    
    Args:
        token: JWT token string
        
    Returns:
        Dict ch·ª©a payload ho·∫∑c None n·∫øu parse failed
    """
    try:
        # JWT structure: header.payload.signature
        parts = token.split('.')
        if len(parts) != 3:
            print(f"[JWT Parser] Invalid token format - expected 3 parts, got {len(parts)}")
            return None
        
        # Decode base64url payload (part 1)
        payload = parts[1]
        # Add padding if needed
        padding = 4 - len(payload) % 4
        if padding != 4:
            payload += '=' * padding
        
        # base64url decode: replace - with + and _ with /
        payload = payload.replace('-', '+').replace('_', '/')
        decoded_bytes = base64.b64decode(payload)
        decoded_str = decoded_bytes.decode('utf-8')
        
        # Parse JSON
        payload_dict = json.loads(decoded_str)
        
        print(f"[JWT Parser] Successfully parsed token - userId: {payload_dict.get('userId')}, role: {payload_dict.get('role')}")
        return payload_dict
        
    except Exception as e:
        print(f"[JWT Parser] Error parsing token: {e}")
        return None

# Configure Gemini API
GEMINI_API_KEY = os.getenv('GOOGLE_API_KEY')
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# Configure Groq API
GROQ_API_KEY = os.getenv('GROQ_API_KEY')
groq_client = None
if GROQ_API_KEY:
    groq_client = Groq(api_key=GROQ_API_KEY)
    
# Cache for models
_cached_models = None
_models_cache_time = None

def resolve_spring_file_path(relative_path):
    """
    Resolve ƒë∆∞·ªùng d·∫´n file t∆∞∆°ng ƒë·ªëi t·ª´ Spring Service th√†nh ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi
    
    Args:
        relative_path: ƒê∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi t·ª´ Spring Service (vd: 'uploads/documents/file.xlsx')
        
    Returns:
        ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi ho·∫∑c None n·∫øu kh√¥ng t√¨m th·∫•y
    """
    if not relative_path:
        return None
    
    # N·∫øu ƒë√£ l√† ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi, tr·∫£ v·ªÅ lu√¥n
    if os.path.isabs(relative_path):
        return relative_path if os.path.exists(relative_path) else None
    
    # C√°c ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ c√≥ c·ªßa Spring Service uploads
    possible_base_paths = [
        # ƒê∆∞·ªùng d·∫´n t·ª´ th∆∞ m·ª•c Python service ƒë·∫øn Spring service
        os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'SpringService', relative_path),
        # ƒê∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi d·ª±a tr√™n c·∫•u tr√∫c project
        os.path.join('/home/hv/DuAn/CSN/AI-Agent-for-Business/backend/SpringService', relative_path),
        # ƒê∆∞·ªùng d·∫´n t·ª´ environment variable n·∫øu c√≥
        os.path.join(os.getenv('SPRING_UPLOAD_PATH', ''), relative_path) if os.getenv('SPRING_UPLOAD_PATH') else None,
    ]
    
    # Th·ª≠ t·ª´ng ƒë∆∞·ªùng d·∫´n c√≥ th·ªÉ
    for base_path in possible_base_paths:
        if base_path and os.path.exists(base_path):
            print(f"[File Resolver] Found file at: {base_path}")
            return base_path
    
    # N·∫øu kh√¥ng t√¨m th·∫•y ·ªü c√°c v·ªã tr√≠ chu·∫©n, th·ª≠ t√¨m trong th∆∞ m·ª•c hi·ªán t·∫°i
    current_dir = os.getcwd()
    fallback_path = os.path.join(current_dir, relative_path)
    if os.path.exists(fallback_path):
        print(f"[File Resolver] Found file at fallback location: {fallback_path}")
        return fallback_path
    
    print(f"[File Resolver] File not found at any location for: {relative_path}")
    return None

def get_available_models_from_apis():
    """Fetch available models from Gemini and Groq APIs"""
    global _cached_models, _models_cache_time
    import time
    
    # Cache for 1 hour
    if _cached_models and _models_cache_time and (time.time() - _models_cache_time) < 3600:
        return _cached_models
    
    models = []
    
    # Get Gemini models - ch·ªâ gi·ªØ Pro v√† Flash 2.5
    if GEMINI_API_KEY:
        try:
            gemini_models = genai.list_models()
            allowed_gemini = ['gemini-2.5-pro', 'gemini-2.5-flash']
            
            for m in gemini_models:
                if 'generateContent' in m.supported_generation_methods:
                    model_id = m.name.replace('models/', '')
                    
                    # Ch·ªâ gi·ªØ Pro v√† Flash 2.5
                    if model_id in allowed_gemini:
                        models.append({
                            'id': model_id,
                            'name': m.display_name,
                            'provider': 'Google',
                            'context_window': getattr(m, 'input_token_limit', 32768)
                        })
                        print(f"[Analytics] Added Gemini model: {model_id}")
            
            print(f"[Analytics] Loaded {len([m for m in models if m['provider'] == 'Google'])} Gemini models")
        except Exception as e:
            print(f"[Analytics] Error loading Gemini models: {e}")
    
    # Get Groq models
    if groq_client:
        try:
            models_response = groq_client.models.list()
            
            # List of keywords to exclude (non-chat models)
            excluded_keywords = ['whisper', 'distil-whisper', 'guard']
            
            groq_models = []
            for model in models_response.data:
                if hasattr(model, 'id') and model.id:
                    model_id_lower = model.id.lower()
                    
                    # Skip non-chat models
                    if any(keyword in model_id_lower for keyword in excluded_keywords):
                        print(f"[Analytics] Skipping excluded model: {model.id}")
                        continue
                    
                    # Only include active models
                    if not getattr(model, 'active', True):
                        print(f"[Analytics] Skipping inactive model: {model.id}")
                        continue
                    
                    context_window = getattr(model, 'context_window', 8192)
                    
                    groq_models.append({
                        'id': model.id,
                        'name': model.id,
                        'provider': 'Groq',
                        'context_window': context_window
                    })
                    print(f"[Analytics] Added Groq model: {model.id} (context: {context_window})")
            
            # Sort by name
            groq_models.sort(key=lambda x: x['name'])
            models.extend(groq_models)
            
            print(f"[Analytics] Loaded {len(groq_models)} Groq models")
        except Exception as e:
            print(f"[Analytics] Error loading Groq models: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("[Analytics] Groq client not initialized - check GROQ_API_KEY")
    
    # Sort by provider and name
    models.sort(key=lambda x: (x['provider'], x['name']))
    
    _cached_models = models
    _models_cache_time = time.time()
    
    return models

@router.get('/models')
async def get_available_models():
    """L·∫•y danh s√°ch models AI c√≥ s·∫µn cho ph√¢n t√≠ch t·ª´ API"""
    models = get_available_models_from_apis()
    return {'success': True, 'models': models}

# ChromaDB client
chroma_client = None

def set_chroma_client(client):
    """Set ChromaDB client"""
    global chroma_client
    chroma_client = client

class AIInsightsRequest(BaseModel):
    type: Optional[str] = 'general'  # general, pricing, inventory, sales
    model: Optional[str] = 'llama-3.3-70b-versatile'  # AI model to use - default to Groq Llama 3.3 70B

def get_business_data():
    """L·∫•y d·ªØ li·ªáu kinh doanh t·ª´ ChromaDB"""
    try:
        if not chroma_client:
            return {'products': [], 'orders': [], 'categories': [], 'discounts': [], 'business_performance': [], 'users': [], 'documents': []}
        
        # L·∫•y collections t·ª´ ChromaDB
        # business_data: products, categories, business_performance, discounts
        # orders_analytics: orders
        try:
            business_collection = chroma_client.get_collection(name="business_data")
            orders_collection = chroma_client.get_collection(name="orders_analytics")
            revenue_collection = chroma_client.get_collection(name="revenue_overview")
        except Exception as e:
            print(f"Error getting collections: {e}")
            return {'products': [], 'orders': [], 'categories': [], 'discounts': [], 'business_performance': [], 'users': [], 'documents': [], 'revenue_overview': []}
        
        # L·∫•y t·∫•t c·∫£ d·ªØ li·ªáu t·ª´ collections (limit l·ªõn ƒë·ªÉ ƒë·∫£m b·∫£o l·∫•y h·∫øt)
        business_data = business_collection.get(include=['metadatas'], limit=10000)
        orders_data = orders_collection.get(include=['metadatas'], limit=10000)
        revenue_data = revenue_collection.get(include=['metadatas'], limit=10000)
        
        # Parse metadata t·ª´ business_collection theo data_type
        all_business_metadatas = business_data.get('metadatas', [])
        
        products = [m for m in all_business_metadatas if m.get('data_type') == 'product']
        categories = [m for m in all_business_metadatas if m.get('data_type') == 'category']
        discounts = [m for m in all_business_metadatas if m.get('data_type') == 'discount']
        business_performance = [m for m in all_business_metadatas if m.get('data_type') == 'business_performance']
        users = [m for m in all_business_metadatas if m.get('data_type') == 'user']
        documents = [m for m in all_business_metadatas if m.get('data_type') == 'document']
        
        # Parse orders t·ª´ orders_analytics collection  
        orders = orders_data.get('metadatas', [])
        
        # Parse revenue overview t·ª´ revenue_overview collection
        revenue_overview = revenue_data.get('metadatas', [])
        
        # Convert string fields back to proper types
        for product in products:
            if 'price' in product and isinstance(product['price'], str):
                try:
                    product['price'] = float(product['price'])
                except:
                    product['price'] = 0
            if 'quantity' in product and isinstance(product['quantity'], str):
                try:
                    product['quantity'] = int(product['quantity'])
                except:
                    product['quantity'] = 0
            if 'total_sold' in product and isinstance(product['total_sold'], str):
                try:
                    product['total_sold'] = int(product['total_sold'])
                except:
                    product['total_sold'] = 0
            if 'totalSold' in product and isinstance(product['totalSold'], str):
                try:
                    product['totalSold'] = int(product['totalSold'])
                except:
                    product['totalSold'] = 0
            if 'total_revenue' in product and isinstance(product['total_revenue'], str):
                try:
                    product['total_revenue'] = float(product['total_revenue'])
                except:
                    product['total_revenue'] = 0
            if 'totalRevenue' in product and isinstance(product['totalRevenue'], str):
                try:
                    product['totalRevenue'] = float(product['totalRevenue'])
                except:
                    product['totalRevenue'] = 0
            if 'id' in product and isinstance(product['id'], str):
                try:
                    product['id'] = int(product['id'])
                except:
                    pass
        
        for order in orders:
            if 'totalAmount' in order and isinstance(order['totalAmount'], str):
                try:
                    order['totalAmount'] = float(order['totalAmount'])
                except:
                    order['totalAmount'] = 0
            if 'id' in order and isinstance(order['id'], str):
                try:
                    order['id'] = int(order['id'])
                except:
                    pass
        
        print(f"[Analytics] Loaded from ChromaDB: {len(products)} products, {len(orders)} orders, {len(categories)} categories, {len(discounts)} discounts, {len(business_performance)} business records, {len(users)} users, {len(documents)} documents")
        
        return {
            'products': products,
            'orders': orders,
            'categories': categories,
            'discounts': discounts,
            'business_performance': business_performance,
            'users': users,
            'documents': documents,
            'revenue_overview': revenue_overview
        }
    except Exception as e:
        print(f"Error fetching business data from ChromaDB: {e}")
        import traceback
        traceback.print_exc()
        return {
            'products': [],
            'orders': [],
            'categories': [],
            'discounts': [],
            'business_performance': [],
            'users': [],
            'documents': [],
            'revenue_overview': []
        }

def calculate_statistics(data):
    """
    T√≠nh to√°n c√°c ch·ªâ s·ªë th·ªëng k√™ v·ªõi forecasting d·ª±a tr√™n k·ªπ thu·∫≠t th·ªëng k√™
    S·ª≠ d·ª•ng: Linear Regression, Exponential Smoothing, Moving Average
    """
    products = data.get('products', [])
    orders = data.get('orders', [])
    categories = data.get('categories', [])
    revenue_overview = data.get('revenue_overview', [])
    
    # Initialize forecasting service
    forecasting = get_forecasting_service()
    
    # Th·ªëng k√™ t·ªïng quan
    total_products = len(products)
    total_orders = len(orders)
    total_categories = len(categories)
    
    # S·ª≠ d·ª•ng d·ªØ li·ªáu doanh thu t·ª´ revenue_overview n·∫øu c√≥, n·∫øu kh√¥ng th√¨ t√≠nh t·ª´ orders
    if revenue_overview:
        # L·∫•y d·ªØ li·ªáu t·ª´ revenue_overview collection
        revenue_data = revenue_overview[0] if revenue_overview else {}
        total_revenue = revenue_data.get('total_revenue', 0)
        monthly_revenue = revenue_data.get('monthly_revenue', 0)
        weekly_revenue = revenue_data.get('weekly_revenue', 0)
        daily_revenue = revenue_data.get('daily_revenue', 0)
    else:
        # Fallback: t√≠nh t·ª´ orders data
        total_revenue = sum(order.get('totalAmount', 0) for order in orders)
        monthly_revenue = 0  # Kh√¥ng th·ªÉ t√≠nh t·ª´ orders data
        weekly_revenue = 0
        daily_revenue = 0
    
    # T√≠nh doanh thu theo tr·∫°ng th√°i
    revenue_by_status = {}
    orders_by_status = {}
    for order in orders:
        status = order.get('status', 'UNKNOWN')
        amount = order.get('totalAmount', 0)
        
        revenue_by_status[status] = revenue_by_status.get(status, 0) + amount
        orders_by_status[status] = orders_by_status.get(status, 0) + 1
    
    # Convert to array format for frontend
    revenue_by_status_array = [
        {'status': status, 'revenue': revenue}
        for status, revenue in revenue_by_status.items()
    ]
    orders_by_status_array = [
        {'status': status, 'count': count}
        for status, count in orders_by_status.items()
    ]
    
    # T√≠nh s·ªë l∆∞·ª£ng ƒë√£ b√°n v√† doanh thu cho t·ª´ng s·∫£n ph·∫©m
    # Note: ChromaDB orders kh√¥ng ch·ª©a chi ti·∫øt items, n√™n d√πng totalSold t·ª´ product metadata
    enriched_products = []
    for product in products:
        # H·ªó tr·ª£ c·∫£ 2 format: totalSold (camelCase) v√† total_sold (snake_case)
        total_sold = product.get('totalSold', product.get('total_sold', 0))
        if isinstance(total_sold, str):
            try:
                total_sold = int(total_sold)
            except:
                total_sold = 0
        
        price = product.get('price', 0)
        if isinstance(price, str):
            try:
                price = float(price)
            except:
                price = 0
        
        # T√≠nh revenue t·ª´ total_sold * price (n·∫øu ch∆∞a c√≥ totalRevenue)
        revenue = product.get('totalRevenue', product.get('total_revenue', 0))
        if isinstance(revenue, str):
            try:
                revenue = float(revenue)
            except:
                revenue = 0
        
        # N·∫øu kh√¥ng c√≥ revenue s·∫µn, t√≠nh t·ª´ total_sold * price
        if revenue == 0 and total_sold > 0:
            revenue = total_sold * price
        
        enriched_product = {
            **product,
            'stock': product.get('quantity', 0),  # ƒê·ªïi quantity -> stock
            'total_sold': total_sold,
            'revenue': revenue
        }
        enriched_products.append(enriched_product)
    
    # Top s·∫£n ph·∫©m b√°n ch·∫°y (theo total_sold v√† revenue)
    products_sorted = sorted(enriched_products, key=lambda x: (x.get('total_sold', 0), x.get('revenue', 0)), reverse=True)
    top_products = products_sorted
    
    # S·∫£n ph·∫©m s·∫Øp h·∫øt h√†ng (stock < 20)
    low_stock_products = sorted(
        [p for p in enriched_products if p.get('stock', 0) < 20],
        key=lambda x: x.get('stock', 0)
    )
    
    # Ph√¢n t√≠ch theo danh m·ª•c
    category_stats = {}
    for product in products:
        cat_id = product.get('categoryId')
        cat_name = product.get('categoryName', 'Unknown')
        
        if cat_name not in category_stats:
            category_stats[cat_name] = {
                'product_count': 0,
                'total_stock': 0,
                'avg_price': 0,
                'total_price': 0
            }
        
        category_stats[cat_name]['product_count'] += 1
        category_stats[cat_name]['total_stock'] += product.get('quantity', 0)
        category_stats[cat_name]['total_price'] += product.get('price', 0)
    
    # T√≠nh gi√° trung b√¨nh theo danh m·ª•c
    for cat_name, stats in category_stats.items():
        if stats['product_count'] > 0:
            stats['avg_price'] = stats['total_price'] / stats['product_count']
    
    # Ph√¢n t√≠ch theo th·ªùi gian (7 ng√†y g·∫ßn nh·∫•t)
    now = datetime.now()
    last_7_days = now - timedelta(days=7)
    
    revenue_by_day = {}
    orders_by_day = {}
    
    for order in orders:
        created_at = order.get('createdAt', '')
        if created_at:
            try:
                order_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                date_key = order_date.strftime('%Y-%m-%d')
                
                revenue_by_day[date_key] = revenue_by_day.get(date_key, 0) + order.get('totalAmount', 0)
                orders_by_day[date_key] = orders_by_day.get(date_key, 0) + 1
            except:
                pass
    
    # T√≠nh available_stock cho t·ª´ng s·∫£n ph·∫©m
    # NOTE: 'quantity' trong CSDL ƒë√£ l√† s·ªë l∆∞·ª£ng t·ªìn kho HI·ªÜN T·∫†I (available stock)
    # Kh√¥ng c·∫ßn tr·ª´ totalSold v√¨ quantity ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t m·ªói khi c√≥ ƒë∆°n h√†ng
    for product in enriched_products:
        quantity = product.get('quantity', 0)
        if isinstance(quantity, str):
            try:
                quantity = int(quantity)
            except:
                quantity = 0
        
        # available_stock ch√≠nh l√† quantity hi·ªán t·∫°i
        product['available_stock'] = max(0, quantity)
    
    # Ph√¢n t√≠ch t·ªìn kho chi ti·∫øt theo y√™u c·∫ßu: ‚â•30, 10-29, 1-9, 0
    total_inventory_value = sum([p.get('price', 0) * p.get('available_stock', 0) for p in enriched_products])
    
    # Categorize products
    stock_good = [p for p in enriched_products if p.get('available_stock', 0) >= 30]
    stock_avg = [p for p in enriched_products if 10 <= p.get('available_stock', 0) < 30]
    stock_low = [p for p in enriched_products if 1 <= p.get('available_stock', 0) < 10]
    stock_out = [p for p in enriched_products if p.get('available_stock', 0) == 0]
    
    total_products_count = len(enriched_products) if enriched_products else 1  # Avoid division by zero
    
    inventory_table_data = {
        'good': {
            'count': len(stock_good),
            'value': sum([p.get('price', 0) * p.get('available_stock', 0) for p in stock_good]),
            'percent': (len(stock_good) / total_products_count) * 100
        },
        'average': {
            'count': len(stock_avg),
            'value': sum([p.get('price', 0) * p.get('available_stock', 0) for p in stock_avg]),
            'percent': (len(stock_avg) / total_products_count) * 100
        },
        'low': {
            'count': len(stock_low),
            'value': sum([p.get('price', 0) * p.get('available_stock', 0) for p in stock_low]),
            'percent': (len(stock_low) / total_products_count) * 100
        },
        'out': {
            'count': len(stock_out),
            'value': 0,
            'percent': (len(stock_out) / total_products_count) * 100
        }
    }
    
    inventory_turnover_ratio = total_revenue / total_inventory_value if total_inventory_value > 0 else 0
    out_of_stock_products = len(stock_out)
    
    # === PH√ÇN T√çCH TƒÇNG TR∆Ø·ªûNG B√ÅN H√ÄNG ===
    growth_analysis = {}
    
    # T√≠nh tƒÉng tr∆∞·ªüng theo th·ªùi gian
    if len(revenue_by_day) >= 14:  # C·∫ßn √≠t nh·∫•t 14 ng√†y ƒë·ªÉ so s√°nh 2 tu·∫ßn
        sorted_dates = sorted(revenue_by_day.keys())
        
        # Chia th√†nh 2 n·ª≠a ƒë·ªÉ so s√°nh
        mid_point = len(sorted_dates) // 2
        first_half_dates = sorted_dates[:mid_point]
        second_half_dates = sorted_dates[mid_point:]
        
        revenue_first_half = sum([revenue_by_day[d] for d in first_half_dates])
        revenue_second_half = sum([revenue_by_day[d] for d in second_half_dates])
        
        orders_first_half = sum([orders_by_day.get(d, 0) for d in first_half_dates])
        orders_second_half = sum([orders_by_day.get(d, 0) for d in second_half_dates])
        
        # T√≠nh % tƒÉng tr∆∞·ªüng
        revenue_growth_rate = ((revenue_second_half - revenue_first_half) / revenue_first_half * 100) if revenue_first_half > 0 else 0
        orders_growth_rate = ((orders_second_half - orders_first_half) / orders_first_half * 100) if orders_first_half > 0 else 0
        
        growth_analysis['revenue_growth'] = {
            'rate': revenue_growth_rate,
            'previous_period': revenue_first_half,
            'current_period': revenue_second_half,
            'trend': 'increasing' if revenue_growth_rate > 0 else 'decreasing' if revenue_growth_rate < 0 else 'stable'
        }
        
        growth_analysis['orders_growth'] = {
            'rate': orders_growth_rate,
            'previous_period': orders_first_half,
            'current_period': orders_second_half,
            'trend': 'increasing' if orders_growth_rate > 0 else 'decreasing' if orders_growth_rate < 0 else 'stable'
        }
        
        # AOV trend
        aov_first = revenue_first_half / orders_first_half if orders_first_half > 0 else 0
        aov_second = revenue_second_half / orders_second_half if orders_second_half > 0 else 0
        aov_growth = ((aov_second - aov_first) / aov_first * 100) if aov_first > 0 else 0
        
        growth_analysis['aov_growth'] = {
            'rate': aov_growth,
            'previous_period': aov_first,
            'current_period': aov_second,
            'trend': 'increasing' if aov_growth > 0 else 'decreasing' if aov_growth < 0 else 'stable'
        }
    
    # === PH√ÇN KH√öC KH√ÅCH H√ÄNG ===
    customer_segments = {}
    
    # Ph√¢n t√≠ch theo kh√°ch h√†ng t·ª´ orders
    customer_data = {}
    for order in orders:
        customer_id = order.get('customer_id', order.get('customerId'))
        customer_name = order.get('customer_name', order.get('customerName', 'Unknown'))
        
        if customer_id not in customer_data:
            customer_data[customer_id] = {
                'name': customer_name,
                'total_orders': 0,
                'total_spent': 0,
                'orders': []
            }
        
        customer_data[customer_id]['total_orders'] += 1
        customer_data[customer_id]['total_spent'] += order.get('totalAmount', order.get('total_amount', 0))
        customer_data[customer_id]['orders'].append(order)
    
    if customer_data:
        # Ph√¢n lo·∫°i kh√°ch h√†ng theo RFM (ƒë∆°n gi·∫£n h√≥a)
        customer_list = list(customer_data.values())
        
        # T√≠nh ng∆∞·ª°ng ph√¢n kh√∫c
        avg_orders = sum([c['total_orders'] for c in customer_list]) / len(customer_list)
        avg_spent = sum([c['total_spent'] for c in customer_list]) / len(customer_list)
        
        vip_customers = [c for c in customer_list if c['total_spent'] >= avg_spent * 2]
        loyal_customers = [c for c in customer_list if c['total_orders'] >= avg_orders * 1.5 and c not in vip_customers]
        regular_customers = [c for c in customer_list if c not in vip_customers and c not in loyal_customers and c['total_orders'] > 1]
        one_time_customers = [c for c in customer_list if c['total_orders'] == 1]
        
        customer_segments = {
            'total_customers': len(customer_list),
            'vip': {
                'count': len(vip_customers),
                'total_revenue': sum([c['total_spent'] for c in vip_customers]),
                'avg_order_value': sum([c['total_spent'] for c in vip_customers]) / sum([c['total_orders'] for c in vip_customers]) if vip_customers else 0,
                'revenue_contribution': (sum([c['total_spent'] for c in vip_customers]) / total_revenue * 100) if total_revenue > 0 else 0
            },
            'loyal': {
                'count': len(loyal_customers),
                'total_revenue': sum([c['total_spent'] for c in loyal_customers]),
                'avg_order_value': sum([c['total_spent'] for c in loyal_customers]) / sum([c['total_orders'] for c in loyal_customers]) if loyal_customers else 0,
                'revenue_contribution': (sum([c['total_spent'] for c in loyal_customers]) / total_revenue * 100) if total_revenue > 0 else 0
            },
            'regular': {
                'count': len(regular_customers),
                'total_revenue': sum([c['total_spent'] for c in regular_customers]),
                'avg_order_value': sum([c['total_spent'] for c in regular_customers]) / sum([c['total_orders'] for c in regular_customers]) if regular_customers else 0,
                'revenue_contribution': (sum([c['total_spent'] for c in regular_customers]) / total_revenue * 100) if total_revenue > 0 else 0
            },
            'one_time': {
                'count': len(one_time_customers),
                'total_revenue': sum([c['total_spent'] for c in one_time_customers]),
                'avg_order_value': sum([c['total_spent'] for c in one_time_customers]) / len(one_time_customers) if one_time_customers else 0,
                'revenue_contribution': (sum([c['total_spent'] for c in one_time_customers]) / total_revenue * 100) if total_revenue > 0 else 0
            }
        }
    
    inventory_analysis = {
        'critical_stock_products': stock_low,  # T·ªìn kho th·∫•p (1-9)
        'warning_stock_products': stock_avg,   # T·ªìn kho trung b√¨nh (10-29)
        'out_of_stock_products': stock_out,    # H·∫øt h√†ng (0)
        'stock_distribution': {
            'well_stocked': {'count': len(stock_good), 'value': inventory_table_data['good']['value']},
            'medium_stock': {'count': len(stock_avg), 'value': inventory_table_data['average']['value']},
            'low_stock': {'count': len(stock_low), 'value': inventory_table_data['low']['value']},
            'out_of_stock': {'count': len(stock_out), 'value': 0}
        },
        'table_data': inventory_table_data
    }
    
    # === FORECASTING D·ª∞A TR√äN K·ª∏ THU·∫¨T TH·ªêNG K√ä ===
    forecast_data = {}
    
    # 1. Revenue Forecasting (7 ng√†y ti·∫øp theo)
    if revenue_by_day and len(revenue_by_day) >= 3:
        try:
            revenue_forecast = forecasting.revenue_forecast(
                revenue_by_day=revenue_by_day,
                periods_ahead=7
            )
            forecast_data['revenue'] = {
                'next_7_days_total': revenue_forecast['total_forecast'],
                'daily_average': revenue_forecast['daily_average'],
                'forecast_by_day': revenue_forecast['forecast_by_day'],
                'trend': revenue_forecast['trend'],
                'confidence': revenue_forecast['confidence'],
                'method': revenue_forecast['method'],
                'historical_daily_avg': revenue_forecast['historical_average']
            }
        except Exception as e:
            print(f"[Forecasting] Revenue forecast error: {e}")
            forecast_data['revenue'] = None
    
    # 2. Inventory Reorder Points (cho s·∫£n ph·∫©m low stock)
    reorder_recommendations = []
    for product in stock_low + stock_out:
        try:
            product_id = product.get('id', product.get('product_id'))
            
            # Tr√≠ch xu·∫•t l·ªãch s·ª≠ b√°n h√†ng TH·ª∞C T·∫æ t·ª´ orders (30 ng√†y)
            sales_history = extract_product_sales_history(orders, product_id, days=30)
            
            # Ki·ªÉm tra c√≥ d·ªØ li·ªáu b√°n h√†ng kh√¥ng
            total_sales = sum(sales_history)
            if total_sales > 0 and len(sales_history) >= 7:
                reorder_calc = forecasting.inventory_reorder_point(
                    sales_history=sales_history,
                    lead_time_days=7,
                    service_level=0.95
                )
                
                current_stock = product.get('available_stock', 0)
                reorder_point = reorder_calc['reorder_point']
                
                reorder_recommendations.append({
                    'product_id': product_id,
                    'product_name': product.get('name'),
                    'current_stock': current_stock,
                    'reorder_point': reorder_point,
                    'safety_stock': reorder_calc['safety_stock'],
                    'avg_daily_sales': round(reorder_calc['average_daily_sales'], 2),
                    'recommended_order_quantity': max(0, reorder_point - current_stock),
                    'urgency': 'high' if current_stock == 0 else 'medium',
                    'days_of_data': len([s for s in sales_history if s > 0])  # S·ªë ng√†y c√≥ b√°n h√†ng
                })
            else:
                # Kh√¥ng ƒë·ªß d·ªØ li·ªáu, d√πng total_sold l√†m fallback
                print(f"[Reorder] Not enough sales data for {product.get('name')} (total_sales={total_sales})")
                
        except Exception as e:
            print(f"[Forecasting] Reorder calc error for product {product.get('name')}: {e}")
            import traceback
            traceback.print_exc()
    
    forecast_data['inventory_reorder'] = reorder_recommendations
    
    # 3. Sales Trend Analysis v·ªõi Linear Regression
    if revenue_by_day and len(revenue_by_day) >= 7:
        try:
            sorted_dates = sorted(revenue_by_day.keys())
            revenue_values = [revenue_by_day[date] for date in sorted_dates]
            
            trend_analysis = forecasting.linear_regression_forecast(
                data=revenue_values,
                periods_ahead=7
            )
            
            forecast_data['trend_analysis'] = {
                'trend_direction': trend_analysis['trend'],
                'growth_rate': trend_analysis['slope'],
                'confidence': trend_analysis['confidence'],
                'next_period_forecast': trend_analysis['forecast'],
                'method': 'linear_regression',
                'interpretation': _interpret_trend(trend_analysis)
            }
        except Exception as e:
            print(f"[Forecasting] Trend analysis error: {e}")
            forecast_data['trend_analysis'] = None
    
    # 4. Product-specific forecasts (top 10 products)
    product_forecasts = []
    for product in top_products[:10]:
        try:
            product_id = product.get('id', product.get('product_id'))
            
            # Tr√≠ch xu·∫•t l·ªãch s·ª≠ b√°n h√†ng TH·ª∞C T·∫æ t·ª´ orders (30 ng√†y)
            sales_history = extract_product_sales_history(orders, product_id, days=30)
            
            # Ki·ªÉm tra c√≥ d·ªØ li·ªáu b√°n h√†ng kh√¥ng
            total_sales = sum(sales_history)
            if total_sales > 0 and len(sales_history) >= 7:
                # D·ª± b√°o daily sales cho 1 ng√†y d·ª±a tr√™n d·ªØ li·ªáu th·ª±c
                ensemble_forecast = forecasting.ensemble_forecast(
                    data=sales_history,
                    periods_ahead=1  # D·ª± b√°o 1 ng√†y
                )
                
                daily_forecast = ensemble_forecast['forecast']
                
                # T√≠nh d·ª± b√°o 7 ng√†y = daily_forecast * 7
                forecast_7days = daily_forecast * 7
                
                # T√≠nh s·ªë ng√†y t·ªìn kho ƒë·ªß d√πng
                available_stock = product.get('available_stock', 0)
                if daily_forecast > 0:
                    stock_coverage_days = int(available_stock / daily_forecast)
                else:
                    # N·∫øu kh√¥ng c√≥ d·ª± b√°o b√°n h√†ng, t·ªìn kho ƒë·ªß d√πng r·∫•t l√¢u
                    stock_coverage_days = 365 if available_stock > 0 else 0
                
                product_forecasts.append({
                    'product_id': product_id,
                    'product_name': product.get('name'),
                    'current_stock': available_stock,
                    'forecast_7day_sales': int(forecast_7days),
                    'daily_forecast': round(daily_forecast, 2),
                    'confidence': ensemble_forecast['confidence'],
                    'stock_coverage_days': stock_coverage_days,
                    'needs_restock': available_stock < forecast_7days,
                    'actual_30day_sales': int(total_sales),  # T·ªïng b√°n th·ª±c t·∫ø 30 ng√†y
                    'days_of_data': len([s for s in sales_history if s > 0])  # S·ªë ng√†y c√≥ b√°n h√†ng
                })
            else:
                # Kh√¥ng ƒë·ªß d·ªØ li·ªáu th·ª±c t·∫ø
                print(f"[Forecast] Not enough sales data for {product.get('name')} (total_sales={total_sales}, history_length={len(sales_history)})")
                
        except Exception as e:
            print(f"[Forecasting] Product forecast error for {product.get('name')}: {e}")
            import traceback
            traceback.print_exc()
    
    forecast_data['product_forecasts'] = sorted(
        product_forecasts, 
        key=lambda x: x['stock_coverage_days']
    )
    
    return {
        'overview': {
            'total_products': total_products,
            'total_orders': total_orders,
            'total_categories': total_categories,
            'total_revenue': total_revenue,
            'monthly_revenue': monthly_revenue,
            'weekly_revenue': weekly_revenue,
            'daily_revenue': daily_revenue,
            'avg_order_value': total_revenue / total_orders if total_orders > 0 else 0,
            'total_inventory_value': total_inventory_value,
            'out_of_stock_products': out_of_stock_products,
            'inventory_turnover_ratio': inventory_turnover_ratio
        },
        'revenue_by_status': revenue_by_status_array,
        'orders_by_status': orders_by_status_array,
        'top_products': top_products,
        'low_stock_products': sorted(stock_low, key=lambda x: x.get('available_stock', 0)),
        'category_stats': category_stats,
        'inventory_analysis': inventory_analysis,
        'revenue_by_day': revenue_by_day,
        'orders_by_day': orders_by_day,
        'growth_analysis': growth_analysis,  # TH√äM PH√ÇN T√çCH TƒÇNG TR∆Ø·ªûNG
        'customer_segments': customer_segments,  # TH√äM PH√ÇN KH√öC KH√ÅCH H√ÄNG
        'forecasts': forecast_data  # TH√äM D·ª∞ B√ÅO TH·ªêNG K√ä
    }

def _interpret_trend(trend_result: Dict[str, Any]) -> str:
    """Interpret trend analysis results"""
    trend = trend_result['trend']
    slope = trend_result['slope']
    confidence = trend_result['confidence']
    
    if confidence < 0.5:
        return f"Xu h∆∞·ªõng {trend} nh∆∞ng ƒë·ªô tin c·∫≠y th·∫•p ({confidence:.1%}). C·∫ßn th√™m d·ªØ li·ªáu."
    elif trend == 'increasing':
        growth_pct = abs(slope) * 30  # 30 days
        return f"Xu h∆∞·ªõng tƒÉng tr∆∞·ªüng {growth_pct:.1f}% d·ª± ki·∫øn trong 30 ng√†y t·ªõi (ƒë·ªô tin c·∫≠y: {confidence:.1%})"
    elif trend == 'decreasing':
        decline_pct = abs(slope) * 30
        return f"Xu h∆∞·ªõng gi·∫£m {decline_pct:.1f}% d·ª± ki·∫øn trong 30 ng√†y t·ªõi (ƒë·ªô tin c·∫≠y: {confidence:.1%})"
    else:
        return f"Xu h∆∞·ªõng ·ªïn ƒë·ªãnh, bi·∫øn ƒë·ªông < 5% (ƒë·ªô tin c·∫≠y: {confidence:.1%})"

def extract_product_sales_history(orders: List[Dict], product_id: Any, days: int = 30) -> List[float]:
    """
    Tr√≠ch xu·∫•t l·ªãch s·ª≠ b√°n h√†ng TH·ª∞C T·∫æ c·ªßa s·∫£n ph·∫©m t·ª´ orders
    
    Args:
        orders: Danh s√°ch ƒë∆°n h√†ng
        product_id: ID s·∫£n ph·∫©m c·∫ßn tr√≠ch xu·∫•t
        days: S·ªë ng√†y l·ªãch s·ª≠ (m·∫∑c ƒë·ªãnh 30 ng√†y)
    
    Returns:
        List s·ªë l∆∞·ª£ng b√°n theo ng√†y (t·ª´ c≈© ƒë·∫øn m·ªõi)
    """
    from datetime import datetime, timedelta
    import json
    
    # T·∫°o dict l∆∞u s·ªë l∆∞·ª£ng b√°n theo ng√†y
    sales_by_date = {}
    now = datetime.now()
    
    # Kh·ªüi t·∫°o t·∫•t c·∫£ c√°c ng√†y v·ªõi 0
    for i in range(days):
        date = (now - timedelta(days=days-i-1)).strftime('%Y-%m-%d')
        sales_by_date[date] = 0
    
    # Duy·ªát qua t·∫•t c·∫£ orders
    for order in orders:
        # Ch·ªâ t√≠nh orders ƒë√£ DELIVERED
        if order.get('status') != 'DELIVERED':
            continue
        
        created_at = order.get('createdAt', order.get('created_at', ''))
        if not created_at:
            continue
        
        try:
            # Parse order date
            order_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
            date_key = order_date.strftime('%Y-%m-%d')
            
            # Ch·ªâ l·∫•y orders trong kho·∫£ng th·ªùi gian
            if date_key not in sales_by_date:
                continue
            
            # L·∫•y items t·ª´ order
            items_json = order.get('items_json', '')
            if items_json:
                try:
                    items = json.loads(items_json) if isinstance(items_json, str) else items_json
                    
                    # T√¨m s·∫£n ph·∫©m trong order items
                    for item in items:
                        item_product_id = item.get('product_id')
                        # So s√°nh ID (convert v·ªÅ string ƒë·ªÉ ƒë·∫£m b·∫£o)
                        if str(item_product_id) == str(product_id):
                            quantity = item.get('quantity', 0)
                            if isinstance(quantity, str):
                                quantity = int(quantity)
                            sales_by_date[date_key] += quantity
                            
                except (json.JSONDecodeError, ValueError, TypeError) as e:
                    print(f"[Sales History] Error parsing items_json: {e}")
                    continue
        except Exception as e:
            print(f"[Sales History] Error processing order: {e}")
            continue
    
    # Convert dict to list (sorted by date)
    sorted_dates = sorted(sales_by_date.keys())
    sales_history = [sales_by_date[date] for date in sorted_dates]
    
    return sales_history

@router.get('/data')
async def get_analytics_data():
    """L·∫•y d·ªØ li·ªáu ph√¢n t√≠ch th·ªëng k√™"""
    try:
        # L·∫•y d·ªØ li·ªáu t·ª´ ChromaDB
        business_data = get_business_data()
        
        # T√≠nh to√°n th·ªëng k√™
        statistics = calculate_statistics(business_data)
        
        return {
            'success': True,
            'data': statistics
        }
        
    except Exception as e:
        print(f"Error in analytics data: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@router.post('/ai-insights')
async def get_ai_insights(request: AIInsightsRequest):
    """S·ª≠ d·ª•ng AI ƒë·ªÉ ph√¢n t√≠ch v√† ƒë·ªÅ xu·∫•t chi·∫øn l∆∞·ª£c kinh doanh v·ªõi RAG t·ª´ documents"""
    try:
        # L·∫•y d·ªØ li·ªáu kinh doanh t·ª´ ChromaDB
        business_data = get_business_data()
        statistics = calculate_statistics(business_data)
        
        # üîç SEARCH BUSINESS DOCUMENTS FOR RELEVANT INFORMATION
        document_context = ""
        if analytics_rag_service:
            try:
                # Search for document content related to the analysis type
                search_query = request.type
                doc_results = analytics_rag_service.search_business_data(
                    query=search_query,
                    n_results=5
                )
                
                if doc_results:
                    document_context = "\\n\\nüìÑ TH√îNG TIN T·ª™ T√ÄI LI·ªÜU DOANH NGHI·ªÜP:\\n"
                    for i, doc in enumerate(doc_results, 1):
                        content = doc.get('content', '')[:1000]  # Limit content length
                        document_context += f"\\n--- T√†i li·ªáu {i} ---\\n{content}\\n"
                    
                    print(f"[AI Insights] Found {len(doc_results)} relevant documents")
                else:
                    print("[AI Insights] No relevant documents found")
                    
            except Exception as e:
                print(f"[AI Insights] Error searching documents: {e}")
        
        # T·∫°o prompt cho AI d·ª±a tr√™n lo·∫°i ph√¢n t√≠ch + document context
        prompt = create_analysis_prompt(request.type, statistics, business_data, document_context)
        
        # Use the selected model from request
        model_name = request.model if request.model else 'llama-3.3-70b-versatile'
        print(f"[Analytics] Using AI model: {model_name}")
        
        # Determine provider based on model name - check if it's a Groq model
        groq_model_prefixes = [
            'llama', 'mixtral', 'gemma', 'openai/gpt-oss', 'moonshotai', 
            'meta-llama', 'qwen', 'groq', 'allam', 'playai'
        ]
        is_groq = any(prefix in model_name.lower() for prefix in groq_model_prefixes)
        
        print(f"[Analytics] Model: {model_name}, Provider: {'Groq' if is_groq else 'Gemini'}")
        
        if is_groq and groq_client:
            # Use Groq API
            print(f"[Analytics] Using Groq API")
            try:
                chat_completion = groq_client.chat.completions.create(
                    messages=[
                        {
                            "role": "user",
                            "content": prompt,
                        }
                    ],
                    model=model_name,
                    temperature=0.7,
                    max_tokens=8192,
                )
                ai_insights = chat_completion.choices[0].message.content
            except Exception as groq_error:
                print(f"[Analytics] Groq API error: {groq_error}")
                # Fallback to Gemini if Groq fails
                print(f"[Analytics] Fallback to Gemini API")
                try:
                    model = genai.GenerativeModel('gemini-2.5-flash')
                    response = model.generate_content(prompt)
                    ai_insights = response.text
                except Exception as gemini_error:
                    print(f"[Analytics] Gemini fallback also failed: {gemini_error}")
                    raise HTTPException(status_code=500, detail=f"Both Groq and Gemini APIs failed. Groq: {groq_error}, Gemini: {gemini_error}")
        else:
            # Use Gemini API
            print(f"[Analytics] Using Gemini API")
            try:
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt)
                ai_insights = response.text
            except Exception as gemini_error:
                print(f"[Analytics] Gemini API error: {gemini_error}")
                raise HTTPException(status_code=500, detail=f"Gemini API error: {gemini_error}")
        
        return {
            'success': True,
            'insights': ai_insights,
            'statistics': statistics,
            'analysis_type': request.type
        }
        
    except Exception as e:
        print(f"Error in AI insights: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

def create_analysis_prompt(analysis_type, statistics, business_data, document_context=""):
    """T·∫°o prompt cho AI d·ª±a tr√™n lo·∫°i ph√¢n t√≠ch v√† th√¥ng tin t·ª´ t√†i li·ªáu"""
    
    overview = statistics.get('overview', {})
    revenue_by_status = statistics.get('revenue_by_status', [])
    orders_by_status = statistics.get('orders_by_status', [])
    category_stats = statistics.get('category_stats', {})
    low_stock_products = statistics.get('low_stock_products', [])
    top_products = statistics.get('top_products', [])
    growth_analysis = statistics.get('growth_analysis', {})
    customer_segments = statistics.get('customer_segments', {})
    
    # L·∫•y d·ªØ li·ªáu b·∫£ng ph√¢n t√≠ch t·ªìn kho pre-calculated
    inventory_analysis = statistics.get('inventory_analysis', {})
    inv_table = inventory_analysis.get('table_data', {})
    
    # Create Markdown Table string explicitly
    inventory_table_md = f"""
| Lo·∫°i | S·ªë l∆∞·ª£ng SP | Gi√° tr·ªã (VNƒê) | T·ª∑ l·ªá % |
| :--- | :---: | :---: | :---: |
| üü¢ T·ªët (‚â•30 SP) | {inv_table.get('good', {}).get('count', 0)} | {inv_table.get('good', {}).get('value', 0):,.0f} | {inv_table.get('good', {}).get('percent', 0):.1f}% |
| üü° Trung b√¨nh (10-29 SP) | {inv_table.get('average', {}).get('count', 0)} | {inv_table.get('average', {}).get('value', 0):,.0f} | {inv_table.get('average', {}).get('percent', 0):.1f}% |
| üî¥ Th·∫•p (1-9 SP) | {inv_table.get('low', {}).get('count', 0)} | {inv_table.get('low', {}).get('value', 0):,.0f} | {inv_table.get('low', {}).get('percent', 0):.1f}% |
| ‚ö´ H·∫øt h√†ng (0) | {inv_table.get('out', {}).get('count', 0)} | {inv_table.get('out', {}).get('value', 0):,.0f} | {inv_table.get('out', {}).get('percent', 0):.1f}% |
"""

    # L·∫•y th√™m d·ªØ li·ªáu chi ti·∫øt
    products = business_data.get('products', [])
    orders = business_data.get('orders', [])
    categories = business_data.get('categories', [])
    discounts = business_data.get('discounts', [])
    business_performance = business_data.get('business_performance', [])
    
    # Ph√¢n t√≠ch s√¢u h∆°n
    total_inventory_value = overview.get('total_inventory_value', 0)
    avg_product_price = sum([p.get('price', 0) for p in products]) / len(products) if products else 0
    products_with_details = [p for p in products if p.get('has_details')]
    out_of_stock_count = overview.get('out_of_stock_products', 0)
    
    base_context = f"""
üéØ B·∫†N L√Ä CHUY√äN GIA PH√ÇN T√çCH KINH DOANH & CHI·∫æN L∆Ø·ª¢C CAO C·∫§P
Nhi·ªám v·ª•: Ph√¢n t√≠ch d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p v√† ƒë∆∞a ra Insights ch√≠nh x√°c.
QUAN TR·ªåNG: TUY·ªÜT ƒê·ªêI KH√îNG T·ª∞ T√çNH TO√ÅN L·∫†I S·ªê LI·ªÜU. H√ÉY S·ª¨ D·ª§NG B·∫¢NG S·ªê LI·ªÜU ƒê√É ƒê∆Ø·ª¢C CUNG C·∫§P D∆Ø·ªöI ƒê√ÇY.

üìä 1Ô∏è‚É£ ƒê√ÅNH GI√Å T√åNH TR·∫†NG T·ªíN KHO HI·ªÜN T·∫†I (D·ªÆ LI·ªÜU CH√çNH X√ÅC):
{inventory_table_md}

üìä D·ªÆ LI·ªÜU KINH DOANH T·ªîNG QUAN KH√ÅC:
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
üì¶ S·∫£n ph·∫©m:
   ‚Ä¢ T·ªïng s·ªë: {overview.get('total_products', 0)} s·∫£n ph·∫©m
   ‚Ä¢ C√≥ th√¥ng tin chi ti·∫øt: {len(products_with_details)} s·∫£n ph·∫©m ({len(products_with_details)/len(products)*100:.1f}% n·∫øu c√≥ s·∫£n ph·∫©m)
   ‚Ä¢ Gi√° trung b√¨nh: {avg_product_price:,.0f} VNƒê
   ‚Ä¢ T·ªïng gi√° tr·ªã h√†ng t·ªìn: {total_inventory_value:,.0f} VNƒê
   ‚Ä¢ S·∫£n ph·∫©m h·∫øt h√†ng: {out_of_stock_count} s·∫£n ph·∫©m
   ‚Ä¢ S·∫£n ph·∫©m s·∫Øp h·∫øt h√†ng: {len(low_stock_products)}

üõí ƒê∆°n h√†ng:
   ‚Ä¢ T·ªïng s·ªë: {overview.get('total_orders', 0)} ƒë∆°n
   ‚Ä¢ T·ªïng doanh thu: {overview.get('total_revenue', 0):,.0f} VNƒê
   ‚Ä¢ Gi√° tr·ªã TB/ƒë∆°n: {overview.get('avg_order_value', 0):,.0f} VNƒê

üìà PH√ÇN T√çCH DOANH THU THEO TR·∫†NG TH√ÅI:
{json.dumps(revenue_by_status, indent=2, ensure_ascii=False)}

üìã PH√ÇN B·ªê ƒê∆†N H√ÄNG THEO TR·∫†NG TH√ÅI:
{json.dumps(orders_by_status, indent=2, ensure_ascii=False)}

üè∑Ô∏è TH·ªêNG K√ä THEO DANH M·ª§C S·∫¢N PH·∫®M:
{json.dumps(category_stats, indent=2, ensure_ascii=False)}

‚≠ê TOP 5 S·∫¢N PH·∫®M N·ªîI B·∫¨T:
{json.dumps([{'t√™n': p.get('name'), 'gi√°': f"{p.get('price', 0):,.0f} VNƒê", 't·ªìn_kho': p.get('available_stock', 0), 'ƒë√£_b√°n': p.get('total_sold', 0)} for p in top_products], indent=2, ensure_ascii=False)}

‚ö†Ô∏è S·∫¢N PH·∫®M C·∫¶N NH·∫¨P H√ÄNG (T·ªìn kho < 10):
{json.dumps([{'t√™n': p.get('name'), 't·ªìn_kho': p.get('available_stock', 0), 'gi√°': f"{p.get('price', 0):,.0f} VNƒê"} for p in low_stock_products], indent=2, ensure_ascii=False)}

üìä PH√ÇN T√çCH T·ªíN KHO CHI TI·∫æT:
   ‚Ä¢ T·ª∑ l·ªá quay v√≤ng h√†ng t·ªìn: {overview.get('inventory_turnover_ratio', 0):.2f}
   ‚Ä¢ S·∫£n ph·∫©m h·∫øt h√†ng: {out_of_stock_count}/{len(products)} ({out_of_stock_count/len(products)*100:.1f}% n·∫øu c√≥ s·∫£n ph·∫©m)
   ‚Ä¢ Gi√° tr·ªã h√†ng t·ªìn kho: {total_inventory_value:,.0f} VNƒê
   ‚Ä¢ S·∫£n ph·∫©m t·ªìn kho th·∫•p: {len(low_stock_products)} s·∫£n ph·∫©m

üí∞ TH√îNG TIN KHUY·∫æN M√ÉI:
   ‚Ä¢ T·ªïng s·ªë ch∆∞∆°ng tr√¨nh: {len(discounts)}
   ‚Ä¢ ƒêang ho·∫°t ƒë·ªông: {len([d for d in discounts if d.get('status') == 'ACTIVE'])}

üè¢ HI·ªÜU SU·∫§T NG∆Ø·ªúI B√ÅN:
   ‚Ä¢ T·ªïng s·ªë ng∆∞·ªùi b√°n: {len(business_performance)}
   ‚Ä¢ T·ªïng doanh thu t·∫•t c·∫£: {sum([bp.get('revenue', 0) for bp in business_performance]):,.0f} VNƒê

üìà PH√ÇN T√çCH TƒÇNG TR∆Ø·ªûNG B√ÅN H√ÄNG:
{json.dumps(growth_analysis, indent=2, ensure_ascii=False, default=str)}

üë• PH√ÇN KH√öC KH√ÅCH H√ÄNG (Customer Segmentation):
{json.dumps(customer_segments, indent=2, ensure_ascii=False, default=str)}

{document_context}
"""

    # Format base_context with actual values
    inventory_analysis = statistics.get('inventory_analysis', {})
    
    # Replace placeholders in base_context
    base_context = base_context.replace('{overview.get(\'total_products\', 0)}', str(overview.get('total_products', 0)))
    base_context = base_context.replace('{len(products_with_details)}', str(len(products_with_details)))
    base_context = base_context.replace('{len(products)*100:.1f}', f"{len(products_with_details)/len(products)*100:.1f}" if products else '0.0')
    base_context = base_context.replace('{avg_product_price:,.0f}', f"{avg_product_price:,.0f}")
    base_context = base_context.replace('{total_inventory_value:,.0f}', f"{total_inventory_value:,.0f}")
    base_context = base_context.replace('{out_of_stock_count}', str(out_of_stock_count))
    base_context = base_context.replace('{len(low_stock_products)}', str(len(low_stock_products)))
    base_context = base_context.replace('{overview.get(\'inventory_turnover_ratio\', 0):.2f}', f"{overview.get('inventory_turnover_ratio', 0):.2f}")
    base_context = base_context.replace('{out_of_stock_count/len(products)*100:.1f}', f"{out_of_stock_count/len(products)*100:.1f}" if products else '0.0')
    base_context = base_context.replace('{total_inventory_value:,.0f}', f"{total_inventory_value:,.0f}")
    base_context = base_context.replace('{len(low_stock_products)}', str(len(low_stock_products)))
    
    # Replace JSON strings
    base_context = base_context.replace('{json.dumps(revenue_by_status, indent=2, ensure_ascii=False)}', json.dumps(revenue_by_status, indent=2, ensure_ascii=False))
    base_context = base_context.replace('{json.dumps(orders_by_status, indent=2, ensure_ascii=False)}', json.dumps(orders_by_status, indent=2, ensure_ascii=False))
    base_context = base_context.replace('{json.dumps(category_stats, indent=2, ensure_ascii=False)}', json.dumps(category_stats, indent=2, ensure_ascii=False))
    base_context = base_context.replace('{json.dumps([{\'t√™n\': p.get(\'name\'), \'gi√°\': f"{p.get(\'price\', 0):,.0f} VNƒê", \'t·ªìn_kho\': p.get(\'available_stock\', 0), \'ƒë√£_b√°n\': p.get(\'total_sold\', 0)} for p in top_products], indent=2, ensure_ascii=False)}', json.dumps([{'t√™n': p.get('name'), 'gi√°': f"{p.get('price', 0):,.0f} VNƒê", 't·ªìn_kho': p.get('available_stock', 0), 'ƒë√£_b√°n': p.get('total_sold', 0)} for p in top_products], indent=2, ensure_ascii=False))
    base_context = base_context.replace('{json.dumps([{\'t√™n\': p.get(\'name\'), \'t·ªìn_kho\': p.get(\'available_stock\', 0), \'gi√°\': f"{p.get(\'price\', 0):,.0f} VNƒê"} for p in low_stock_products], indent=2, ensure_ascii=False)}', json.dumps([{'t√™n': p.get('name'), 't·ªìn_kho': p.get('available_stock', 0), 'gi√°': f"{p.get('price', 0):,.0f} VNƒê"} for p in low_stock_products], indent=2, ensure_ascii=False))
    
    # Replace other placeholders
    base_context = base_context.replace('{len(discounts)}', str(len(discounts)))
    base_context = base_context.replace('{len([d for d in discounts if d.get(\'status\') == \'ACTIVE\'])}', str(len([d for d in discounts if d.get('status') == 'ACTIVE'])))
    base_context = base_context.replace('{len(business_performance)}', str(len(business_performance)))
    base_context = base_context.replace('{sum([bp.get(\'revenue\', 0) for bp in business_performance]):,.0f}', f"{sum([bp.get('revenue', 0) for bp in business_performance]):,.0f}")
    base_context = base_context.replace('{document_context}', document_context)

    if analysis_type == 'general':
        prompt = base_context + """

üéØ NHI·ªÜM V·ª§: B√ÅO C√ÅO PH√ÇN T√çCH KINH DOANH CHUY√äN NGHI·ªÜP & CHI·∫æN L∆Ø·ª¢C TƒÇNG TR∆Ø·ªûNG

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìã C·∫§U TR√öC B√ÅO C√ÅO Y√äU C·∫¶U:

## üìä EXECUTIVE SUMMARY (T√≥m t·∫Øt ƒëi·ªÅu h√†nh)
> Vi·∫øt 1 ƒëo·∫°n ng·∫Øn g·ªçn (3-4 c√¢u) t√≥m t·∫Øt t√¨nh h√¨nh kinh doanh hi·ªán t·∫°i, highlight 2-3 insights quan tr·ªçng nh·∫•t v√† 1-2 h√†nh ƒë·ªông ∆∞u ti√™n cao nh·∫•t.

---

## üìà DASHBOARD CH√çNH - CH·ªà S·ªê QUAN TR·ªåNG

T·∫°o b·∫£ng KPIs v·ªõi ƒë√°nh gi√° v√† xu h∆∞·ªõng:

| Ch·ªâ s·ªë | Gi√° tr·ªã hi·ªán t·∫°i | ƒê√°nh gi√° | Xu h∆∞·ªõng | H√†nh ƒë·ªông |
|--------|------------------|----------|----------|-----------|
| üí∞ T·ªïng doanh thu | [X] VNƒê | üü¢/üü°/üî¥ | ‚ÜóÔ∏è/‚ÜòÔ∏è/‚Üí | [G·ª£i √Ω ng·∫Øn] |
| üõí T·ªïng ƒë∆°n h√†ng | [X] ƒë∆°n | üü¢/üü°/üî¥ | ‚ÜóÔ∏è/‚ÜòÔ∏è/‚Üí | [G·ª£i √Ω ng·∫Øn] |
| üíµ Gi√° tr·ªã TB/ƒë∆°n (AOV) | [X] VNƒê | üü¢/üü°/üî¥ | ‚ÜóÔ∏è/‚ÜòÔ∏è/‚Üí | [G·ª£i √Ω ng·∫Øn] |
| üì¶ T·ª∑ l·ªá h√†ng t·ªìn kh·ªèe | [X]% | üü¢/üü°/üî¥ | ‚ÜóÔ∏è/‚ÜòÔ∏è/‚Üí | [G·ª£i √Ω ng·∫Øn] |
| ‚ö†Ô∏è S·∫£n ph·∫©m c·∫ßn nh·∫≠p | [X] SP | üü¢/üü°/üî¥ | ‚ÜóÔ∏è/‚ÜòÔ∏è/‚Üí | [G·ª£i √Ω ng·∫Øn] |
| üîÑ T·ª∑ l·ªá quay v√≤ng h√†ng | [X] l·∫ßn | üü¢/üü°/üî¥ | ‚ÜóÔ∏è/‚ÜòÔ∏è/‚Üí | [G·ª£i √Ω ng·∫Øn] |

**Ch√∫ th√≠ch:** üü¢ T·ªët | üü° C·∫ßn c·∫£i thi·ªán | üî¥ C·∫£nh b√°o | ‚ÜóÔ∏è TƒÉng | ‚ÜòÔ∏è Gi·∫£m | ‚Üí ·ªîn ƒë·ªãnh

---

## üéØ PH√ÇN T√çCH SWOT CHUY√äN S√ÇU

### üí™ ƒêI·ªÇM M·∫†NH (Strengths)
1. **[ƒêi·ªÉm m·∫°nh 1]**: [M√¥ t·∫£ chi ti·∫øt v·ªõi s·ªë li·ªáu c·ª• th·ªÉ]
   - T√°c ƒë·ªông: [ƒê·ªãnh l∆∞·ª£ng impact]
   - C√°ch t·∫≠n d·ª•ng: [G·ª£i √Ω c·ª• th·ªÉ]

2. **[ƒêi·ªÉm m·∫°nh 2]**: [M√¥ t·∫£ chi ti·∫øt v·ªõi s·ªë li·ªáu c·ª• th·ªÉ]
   - T√°c ƒë·ªông: [ƒê·ªãnh l∆∞·ª£ng impact]
   - C√°ch t·∫≠n d·ª•ng: [G·ª£i √Ω c·ª• th·ªÉ]

[Li·ªát k√™ 3-5 ƒëi·ªÉm m·∫°nh]

### ‚ö†Ô∏è ƒêI·ªÇM Y·∫æU (Weaknesses)
1. **[ƒêi·ªÉm y·∫øu 1]**: [M√¥ t·∫£ chi ti·∫øt v·ªõi s·ªë li·ªáu c·ª• th·ªÉ]
   - R·ªßi ro: [ƒê·ªãnh l∆∞·ª£ng risk]
   - Gi·∫£i ph√°p: [H√†nh ƒë·ªông c·ª• th·ªÉ]

2. **[ƒêi·ªÉm y·∫øu 2]**: [M√¥ t·∫£ chi ti·∫øt v·ªõi s·ªë li·ªáu c·ª• th·ªÉ]
   - R·ªßi ro: [ƒê·ªãnh l∆∞·ª£ng risk]
   - Gi·∫£i ph√°p: [H√†nh ƒë·ªông c·ª• th·ªÉ]

[Li·ªát k√™ 3-5 ƒëi·ªÉm y·∫øu]

### üöÄ C∆† H·ªòI (Opportunities)
1. **[C∆° h·ªôi 1]**: [M√¥ t·∫£ c∆° h·ªôi th·ªã tr∆∞·ªùng/n·ªôi b·ªô]
   - Ti·ªÅm nƒÉng: [Doanh thu/l·ª£i nhu·∫≠n d·ª± ki·∫øn]
   - C√°ch khai th√°c: [Chi·∫øn thu·∫≠t c·ª• th·ªÉ]

[Li·ªát k√™ 3-4 c∆° h·ªôi]

### üõ°Ô∏è TH√ÅCH TH·ª®C (Threats)
1. **[Th√°ch th·ª©c 1]**: [M√¥ t·∫£ r·ªßi ro/th√°ch th·ª©c]
   - M·ª©c ƒë·ªô: Cao/Trung b√¨nh/Th·∫•p
   - Ph√≤ng ng·ª´a: [Bi·ªán ph√°p c·ª• th·ªÉ]

[Li·ªát k√™ 2-3 th√°ch th·ª©c]

---

## üéØ CHI·∫æN L∆Ø·ª¢C H√ÄNH ƒê·ªòNG ∆ØU TI√äN (Action Plan)

### Ma tr·∫≠n ∆∞u ti√™n (Priority Matrix):

| H√†nh ƒë·ªông | T√°c ƒë·ªông | ƒê·ªô kh√≥ | ∆Øu ti√™n | Timeline | Chi ph√≠ | ROI d·ª± ki·∫øn |
|-----------|----------|--------|---------|----------|---------|-------------|
| [H√†nh ƒë·ªông 1] | Cao/TB/Th·∫•p | D·ªÖ/TB/Kh√≥ | üî¥ P0 | [X tu·∫ßn] | [Y] VNƒê | [Z]% |
| [H√†nh ƒë·ªông 2] | Cao/TB/Th·∫•p | D·ªÖ/TB/Kh√≥ | üü° P1 | [X tu·∫ßn] | [Y] VNƒê | [Z]% |
| [H√†nh ƒë·ªông 3] | Cao/TB/Th·∫•p | D·ªÖ/TB/Kh√≥ | üü¢ P2 | [X tu·∫ßn] | [Y] VNƒê | [Z]% |

**Ch√∫ th√≠ch:** üî¥ P0 = Kh·∫©n c·∫•p (l√†m ngay) | üü° P1 = Quan tr·ªçng (1-2 tu·∫ßn) | üü¢ P2 = C·∫ßn thi·∫øt (1 th√°ng)

### üìã Chi ti·∫øt t·ª´ng h√†nh ƒë·ªông:

#### üî¥ H√ÄNH ƒê·ªòNG ∆ØU TI√äN CAO (P0) - Th·ª±c hi·ªán ngay

**1. [T√™n h√†nh ƒë·ªông c·ª• th·ªÉ]**
- **M·ª•c ti√™u**: [M·ª•c ti√™u SMART c·ª• th·ªÉ]
- **L√Ω do**: [T·∫°i sao c·∫ßn l√†m ngay]
- **C√°c b∆∞·ªõc th·ª±c hi·ªán**:
  1. [B∆∞·ªõc 1 c·ª• th·ªÉ]
  2. [B∆∞·ªõc 2 c·ª• th·ªÉ]
  3. [B∆∞·ªõc 3 c·ª• th·ªÉ]
- **Ngu·ªìn l·ª±c c·∫ßn**: [Con ng∆∞·ªùi, ng√¢n s√°ch, c√¥ng c·ª•]
- **KPI ƒëo l∆∞·ªùng**: [Ch·ªâ s·ªë c·ª• th·ªÉ ƒë·ªÉ ƒëo th√†nh c√¥ng]
- **K·∫øt qu·∫£ k·ª≥ v·ªçng**: [S·ªë li·ªáu c·ª• th·ªÉ]

[Li·ªát k√™ 2-3 h√†nh ƒë·ªông P0]

#### üü° H√ÄNH ƒê·ªòNG QUAN TR·ªåNG (P1) - Th·ª±c hi·ªán trong 1-2 tu·∫ßn

**1. [T√™n h√†nh ƒë·ªông]**
- **M·ª•c ti√™u**: [SMART goal]
- **C√°c b∆∞·ªõc**: [Li·ªát k√™ ng·∫Øn g·ªçn]
- **KPI**: [Ch·ªâ s·ªë ƒëo l∆∞·ªùng]
- **K·∫øt qu·∫£ k·ª≥ v·ªçng**: [S·ªë li·ªáu]

[Li·ªát k√™ 2-3 h√†nh ƒë·ªông P1]

#### üü¢ H√ÄNH ƒê·ªòNG C·∫¶N THI·∫æT (P2) - L√™n k·∫ø ho·∫°ch trong th√°ng

**1. [T√™n h√†nh ƒë·ªông]**
- **M·ª•c ti√™u**: [SMART goal]
- **K·∫øt qu·∫£ k·ª≥ v·ªçng**: [S·ªë li·ªáu]

[Li·ªát k√™ 2-3 h√†nh ƒë·ªông P2]

---

## üìä PH√ÇN T√çCH THEO Lƒ®NH V·ª∞C

### üí∞ DOANH THU & L·ª¢I NHU·∫¨N
- **Ph√¢n t√≠ch hi·ªán tr·∫°ng**: [ƒê√°nh gi√° chi ti·∫øt]
- **Danh m·ª•c ƒë√≥ng g√≥p nhi·ªÅu nh·∫•t**: [Top 3 v·ªõi % ƒë√≥ng g√≥p]
- **C∆° h·ªôi tƒÉng tr∆∞·ªüng**: [G·ª£i √Ω c·ª• th·ªÉ v·ªõi s·ªë li·ªáu]
- **H√†nh ƒë·ªông ƒë·ªÅ xu·∫•t**: [2-3 h√†nh ƒë·ªông]

### üì¶ T·ªíN KHO & LOGISTICS
- **T√¨nh tr·∫°ng t·ªìn kho**: [ƒê√°nh gi√° d·ª±a tr√™n b·∫£ng ph√¢n lo·∫°i ƒë√£ cung c·∫•p]
- **V·∫•n ƒë·ªÅ c·∫•p b√°ch**: [S·∫£n ph·∫©m h·∫øt h√†ng, t·ªìn kho th·∫•p]
- **T·ªëi ∆∞u h√≥a**: [ƒê·ªÅ xu·∫•t c·ª• th·ªÉ]
- **H√†nh ƒë·ªông ƒë·ªÅ xu·∫•t**: [2-3 h√†nh ƒë·ªông]

### üéØ MARKETING & B√ÅN H√ÄNG
- **Hi·ªáu qu·∫£ hi·ªán t·∫°i**: [ƒê√°nh gi√° conversion, AOV]
- **S·∫£n ph·∫©m ti·ªÅm nƒÉng**: [Top products c·∫ßn ƒë·∫©y m·∫°nh]
- **Chi·∫øn d·ªãch ƒë·ªÅ xu·∫•t**: [2-3 chi·∫øn d·ªãch c·ª• th·ªÉ]
- **H√†nh ƒë·ªông ƒë·ªÅ xu·∫•t**: [2-3 h√†nh ƒë·ªông]

### üë• KH√ÅCH H√ÄNG & TR·∫¢I NGHI·ªÜM
- **Ph√¢n t√≠ch h√†nh vi**: [Insights t·ª´ d·ªØ li·ªáu ƒë∆°n h√†ng]
- **C∆° h·ªôi tƒÉng retention**: [G·ª£i √Ω c·ª• th·ªÉ]
- **H√†nh ƒë·ªông ƒë·ªÅ xu·∫•t**: [2-3 h√†nh ƒë·ªông]

---

## üóìÔ∏è ROADMAP TRI·ªÇN KHAI (Implementation Timeline)

### üöÄ TU·∫¶N 1-2 (Quick Wins)
- [ ] [H√†nh ƒë·ªông 1 - P0]
- [ ] [H√†nh ƒë·ªông 2 - P0]
- [ ] [H√†nh ƒë·ªông 3 - P0]
- **M·ª•c ti√™u**: [K·∫øt qu·∫£ c·ª• th·ªÉ k·ª≥ v·ªçng]

### üìà TH√ÅNG 1 (Foundation)
- [ ] [H√†nh ƒë·ªông 1 - P1]
- [ ] [H√†nh ƒë·ªông 2 - P1]
- [ ] [H√†nh ƒë·ªông 3 - P1]
- **M·ª•c ti√™u**: [K·∫øt qu·∫£ c·ª• th·ªÉ k·ª≥ v·ªçng]

### üéØ TH√ÅNG 2-3 (Growth)
- [ ] [H√†nh ƒë·ªông 1 - P2]
- [ ] [H√†nh ƒë·ªông 2 - P2]
- **M·ª•c ti√™u**: [K·∫øt qu·∫£ c·ª• th·ªÉ k·ª≥ v·ªçng]

### üöÄ QU√ù 2-4 (Scale)
- [ ] [Chi·∫øn l∆∞·ª£c d√†i h·∫°n 1]
- [ ] [Chi·∫øn l∆∞·ª£c d√†i h·∫°n 2]
- **M·ª•c ti√™u**: [K·∫øt qu·∫£ c·ª• th·ªÉ k·ª≥ v·ªçng]

---

## üìä KPI DASHBOARD ƒê·ªÄ XU·∫§T THEO D√ïI

### üìÖ Theo d√µi H√ÄNG TU·∫¶N:
1. **Doanh thu tu·∫ßn**: Target [X] VNƒê
2. **S·ªë ƒë∆°n h√†ng**: Target [Y] ƒë∆°n
3. **AOV (Gi√° tr·ªã TB/ƒë∆°n)**: Target [Z] VNƒê
4. **T·ª∑ l·ªá chuy·ªÉn ƒë·ªïi**: Target [W]%
5. **S·∫£n ph·∫©m h·∫øt h√†ng**: Alert n·∫øu > [N] s·∫£n ph·∫©m

### üìÖ Theo d√µi H√ÄNG TH√ÅNG:
1. **TƒÉng tr∆∞·ªüng doanh thu MoM**: Target +[X]%
2. **T·ª∑ l·ªá quay v√≤ng h√†ng t·ªìn**: Target [Y] l·∫ßn/th√°ng
3. **T·ª∑ l·ªá h√†ng t·ªìn kh·ªèe m·∫°nh**: Target > [Z]%
4. **Customer Retention Rate**: Target [W]%
5. **Gross Margin**: Target [V]%

### üéØ M·ª•c ti√™u QUARTERLY:
- **TƒÉng tr∆∞·ªüng doanh thu**: +[X]% so v·ªõi qu√Ω tr∆∞·ªõc
- **T·ªëi ∆∞u chi ph√≠ v·∫≠n h√†nh**: Gi·∫£m [Y]%
- **M·ªü r·ªông danh m·ª•c**: Th√™m [Z] s·∫£n ph·∫©m m·ªõi
- **TƒÉng customer base**: +[W] kh√°ch h√†ng m·ªõi

---

## üí° K·∫æT LU·∫¨N & KHUY·∫æN NGH·ªä CHI·∫æN L∆Ø·ª¢C

### üéØ 3 ∆Øu ti√™n h√†ng ƒë·∫ßu:
1. **[∆Øu ti√™n 1]**: [M√¥ t·∫£ ng·∫Øn g·ªçn t·∫°i sao quan tr·ªçng]
2. **[∆Øu ti√™n 2]**: [M√¥ t·∫£ ng·∫Øn g·ªçn t·∫°i sao quan tr·ªçng]
3. **[∆Øu ti√™n 3]**: [M√¥ t·∫£ ng·∫Øn g·ªçn t·∫°i sao quan tr·ªçng]

### üìà D·ª± b√°o tƒÉng tr∆∞·ªüng (n·∫øu th·ª±c hi·ªán ƒë·∫ßy ƒë·ªß):
- **Doanh thu**: TƒÉng [X]% trong 3 th√°ng t·ªõi
- **L·ª£i nhu·∫≠n**: TƒÉng [Y]% 
- **Hi·ªáu qu·∫£ v·∫≠n h√†nh**: C·∫£i thi·ªán [Z]%
- **S·ª©c kh·ªèe t·ªìn kho**: ƒê·∫°t [W]% h√†ng t·ªìn kh·ªèe m·∫°nh

### ‚ö†Ô∏è R·ªßi ro c·∫ßn l∆∞u √Ω:
1. [R·ªßi ro 1] - Bi·ªán ph√°p ph√≤ng ng·ª´a: [...]
2. [R·ªßi ro 2] - Bi·ªán ph√°p ph√≤ng ng·ª´a: [...]

---

‚ö° **Y√äU C·∫¶U FORMAT:**
- S·ª≠ d·ª•ng emoji ph√π h·ª£p, b·∫£ng markdown chuy√™n nghi·ªáp
- S·ªë li·ªáu C·ª§ TH·ªÇ v·ªõi ƒë∆°n v·ªã VNƒê, %, th·ªùi gian r√µ r√†ng
- M·ªói ƒë·ªÅ xu·∫•t ph·∫£i c√≥: M·ª•c ti√™u + C√°ch l√†m + KPI ƒëo l∆∞·ªùng + Timeline
- Vi·∫øt ti·∫øng Vi·ªát chuy√™n nghi·ªáp, s√∫c t√≠ch, d·ªÖ hi·ªÉu
- ƒê·ªô d√†i: 1200-1800 t·ª´
- ∆Øu ti√™n ACTIONABLE insights h∆°n l√† m√¥ t·∫£ chung chung
"""

    elif analysis_type == 'pricing':
        prompt = base_context + """

üí∞ NHI·ªÜM V·ª§: PH√ÇN T√çCH CHI·∫æN L∆Ø·ª¢C GI√Å & T·ªêI ∆ØU L·ª¢I NHU·∫¨N

üìù Y√äU C·∫¶U PH√ÇN T√çCH:

## 1Ô∏è‚É£ PH√ÇN T√çCH GI√Å HI·ªÜN T·∫†I
- ƒê√°nh gi√° m·ª©c gi√° c·ªßa t·ª´ng danh m·ª•c s·∫£n ph·∫©m
- So s√°nh gi√° trung b√¨nh v·ªõi th·ªã tr∆∞·ªùng (n·∫øu c√≥ th√¥ng tin)
- Ph√¢n t√≠ch kho·∫£ng gi√°: th·∫•p, trung b√¨nh, cao
- Price elasticity: s·∫£n ph·∫©m n√†o nh·∫°y c·∫£m v·ªõi gi√°?

## 2Ô∏è‚É£ C∆† H·ªòI TƒÇNG GI√Å üìà
T·∫°o b·∫£ng markdown:
| S·∫£n ph·∫©m/Danh m·ª•c | Gi√° hi·ªán t·∫°i | ƒê·ªÅ xu·∫•t | L√Ω do | T√°c ƒë·ªông d·ª± ki·∫øn |
|-------------------|--------------|---------|-------|------------------|

### ƒêi·ªÅu ki·ªán ƒë·ªÉ tƒÉng gi√° th√†nh c√¥ng:
- [Li·ªát k√™ 3-5 ƒëi·ªÅu ki·ªán c·ª• th·ªÉ]

## 3Ô∏è‚É£ C∆† H·ªòI GI·∫¢M GI√Å/KHUY·∫æN M√ÉI üìâ
T·∫°o b·∫£ng markdown:
| S·∫£n ph·∫©m/Danh m·ª•c | Gi√° hi·ªán t·∫°i | ƒê·ªÅ xu·∫•t | M·ª•c ti√™u | ROI d·ª± ki·∫øn |
|-------------------|--------------|---------|----------|-------------|

## 4Ô∏è‚É£ CHI·∫æN L∆Ø·ª¢C COMBO & BUNDLE üéÅ

üö® **QUY T·∫ÆC B·∫ÆT BU·ªòC KHI ƒê·ªÄ XU·∫§T COMBO:**

### ‚ùå C·∫§M TUY·ªÜT ƒê·ªêI:
1. **KH√îNG ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t s·∫£n ph·∫©m KH√îNG C√ì trong danh s√°ch TOP 5 s·∫£n ph·∫©m ho·∫∑c danh m·ª•c ƒë√£ cung c·∫•p**
   - CH·ªà s·ª≠ d·ª•ng s·∫£n ph·∫©m t·ª´ d·ªØ li·ªáu th·ª±c t·∫ø ph√≠a tr√™n
   - KH√îNG t·ª± nghƒ© ra t√™n s·∫£n ph·∫©m (VD: "Chu·ªôt Logitech", "Balo laptop")
   - N·∫øu kh√¥ng c√≥ ph·ª• ki·ªán ‚Üí KH√îNG ƒë·ªÅ xu·∫•t combo

2. **KH√îNG combo 2 s·∫£n ph·∫©m C√ôNG CH·ª®C NƒÇNG**
   - VD SAI: MacBook + Laptop Dell (2 laptop)
   - VD SAI: iPhone + Samsung Galaxy (2 ƒëi·ªán tho·∫°i)
   - VD SAI: Tai nghe Sony + Tai nghe AirPods

### ‚úÖ CH·ªà ƒê·ªÄ XU·∫§T KHI:
1. **C√≥ S·∫¢N PH·∫®M TH·ª∞C T·∫æ trong d·ªØ li·ªáu:**
   - Ki·ªÉm tra danh s√°ch TOP 5 s·∫£n ph·∫©m
   - Ki·ªÉm tra danh m·ª•c s·∫£n ph·∫©m
   - Ch·ªâ gh√©p nh·ªØng s·∫£n ph·∫©m ƒê√É T·ªíN T·∫†I

2. **Logic h·ª£p l√Ω - B·ªï sung/H·ªó tr·ª£:**
   - S·∫£n ph·∫©m ch√≠nh + Ph·ª• ki·ªán (n·∫øu c√≥ trong data)
   - Thi·∫øt b·ªã + B·∫£o v·ªá (n·∫øu c√≥ trong data)
   - Complementary products (n·∫øu c√≥ trong data)

### Combo ƒë·ªÅ xu·∫•t (D·ª∞A V√ÄO D·ªÆ LI·ªÜU TH·ª∞C T·∫æ):

‚ö†Ô∏è **TR∆Ø·ªöC KHI ƒê·ªÄ XU·∫§T - KI·ªÇM TRA:**
- [ ] T·∫•t c·∫£ s·∫£n ph·∫©m trong combo c√≥ trong TOP 5 ho·∫∑c danh m·ª•c?
- [ ] Kh√¥ng ph·∫£i 2 s·∫£n ph·∫©m c√πng ch·ª©c nƒÉng?
- [ ] Logic h·ª£p l√Ω cho kh√°ch h√†ng?

**N·∫æU KH√îNG ƒê·ª¶ D·ªÆ LI·ªÜU PH·ª§ KI·ªÜN ‚Üí VI·∫æT:**
"‚ö†Ô∏è Hi·ªán t·∫°i kh√¥ng ƒë·ªß d·ªØ li·ªáu v·ªÅ ph·ª• ki·ªán/s·∫£n ph·∫©m b·ªï sung ƒë·ªÉ ƒë·ªÅ xu·∫•t combo h·ª£p l√Ω. 
Khuy·∫øn ngh·ªã: B·ªï sung th√™m s·∫£n ph·∫©m ph·ª• ki·ªán (chu·ªôt, balo, ·ªëp l∆∞ng, tai nghe...) ƒë·ªÉ tƒÉng AOV qua combo."

**N·∫æU C√ì ƒê·ª¶ D·ªÆ LI·ªÜU ‚Üí ƒê·ªÄ XU·∫§T:**
1. **[T√™n combo t·ª´ DATA]**: [S·∫£n ph·∫©m A t·ª´ TOP 5] + [S·∫£n ph·∫©m B t·ª´ danh m·ª•c]
   - S·∫£n ph·∫©m: [T√™n CH√çNH X√ÅC t·ª´ d·ªØ li·ªáu]
   - Gi√° l·∫ª: [X] VNƒê (t√≠nh t·ª´ gi√° th·ª±c t·∫ø)
   - Gi√° combo: [Y] VNƒê (gi·∫£m 10-15%)
   - L√Ω do h·ª£p l√Ω: [Gi·∫£i th√≠ch use case]
   - V√≠ d·ª•: "Kh√°ch mua [s·∫£n ph·∫©m A] th∆∞·ªùng c·∫ßn [s·∫£n ph·∫©m B] ƒë·ªÉ..."

[ƒê·ªÅ xu·∫•t t·ªëi ƒëa 3-5 combo - CH·ªà T·ª™ D·ªÆ LI·ªÜU C√ì S·∫¥N]

## 5Ô∏è‚É£ L·ªäCH KHUY·∫æN M√ÉI ƒê·ªÄ XU·∫§T üìÖ
T·∫°o b·∫£ng markdown:
| Th·ªùi ƒëi·ªÉm | Lo·∫°i KM | S·∫£n ph·∫©m | M·ª©c gi·∫£m | M·ª•c ti√™u | Budget |
|-----------|---------|----------|----------|----------|--------|

## 6Ô∏è‚É£ CHI·∫æN THU·∫¨T GI√Å T√ÇM L√ù üß†
- **Psychological Pricing**: Gi√° l·∫ª (999,000 thay v√¨ 1,000,000)
- **Anchor Pricing**: Hi·ªÉn th·ªã gi√° g·ªëc ƒë·ªÉ t·∫°o gi√° tr·ªã
- **Premium Pricing**: S·∫£n ph·∫©m cao c·∫•p ƒë·ªãnh v·ªã gi√° cao
- **Loss Leader**: S·∫£n ph·∫©m thu h√∫t v·ªõi gi√° th·∫•p

## 7Ô∏è‚É£ D·ª∞ √ÅN TƒÇNG DOANH THU V√Ä L·ª¢I NHU·∫¨N
- TƒÉng doanh thu d·ª± ki·∫øn: **+[X]%**
- TƒÉng l·ª£i nhu·∫≠n d·ª± ki·∫øn: **+[Y]%**
- TƒÉng AOV d·ª± ki·∫øn: **+[Z]%**
- Timeline th·ª±c hi·ªán: [3-6 th√°ng]
- Ng√¢n s√°ch c·∫ßn: [X] VNƒê
- ROI expected: [Y]X

‚ö° Vi·∫øt chi ti·∫øt v·ªõi s·ªë li·ªáu c·ª• th·ªÉ, d·ªÖ √°p d·ª•ng ngay!
"""

    elif analysis_type == 'inventory':
        prompt = base_context + f"""

üì¶ NHI·ªÜM V·ª§: PH√ÇN T√çCH & T·ªêI ∆ØU QU·∫¢N L√ù T·ªíN KHO

üìù Y√äU C·∫¶U PH√ÇN T√çCH:

## 1Ô∏è‚É£ ƒê√ÅNH GI√Å T√åNH TR·∫†NG T·ªíN KHO HI·ªÜN T·∫†I
### üìä Ph√¢n lo·∫°i t·ªìn kho:
T·∫°o b·∫£ng markdown v·ªõi d·ªØ li·ªáu th·ª±c t·∫ø:
| Lo·∫°i | S·ªë l∆∞·ª£ng SP | Gi√° tr·ªã | T·ª∑ l·ªá % |
|------|-------------|---------|---------|
| üü¢ T·ªët (‚â•30 SP) | {inventory_analysis.get('stock_distribution', {}).get('well_stocked', {}).get('count', 0)} | {inventory_analysis.get('stock_distribution', {}).get('well_stocked', {}).get('value', 0):,.0f} VNƒê | {inventory_analysis.get('stock_distribution', {}).get('well_stocked', {}).get('count', 0)/overview.get('total_products', 1)*100:.1f}% |
| üü° Trung b√¨nh (10-29) | {inventory_analysis.get('stock_distribution', {}).get('medium_stock', {}).get('count', 0)} | {inventory_analysis.get('stock_distribution', {}).get('medium_stock', {}).get('value', 0):,.0f} VNƒê | {inventory_analysis.get('stock_distribution', {}).get('medium_stock', {}).get('count', 0)/overview.get('total_products', 1)*100:.1f}% |
| üî¥ Th·∫•p (1-9) | {inventory_analysis.get('stock_distribution', {}).get('low_stock', {}).get('count', 0)} | {inventory_analysis.get('stock_distribution', {}).get('low_stock', {}).get('value', 0):,.0f} VNƒê | {inventory_analysis.get('stock_distribution', {}).get('low_stock', {}).get('count', 0)/overview.get('total_products', 1)*100:.1f}% |
| ‚ö´ H·∫øt h√†ng (0) | {inventory_analysis.get('stock_distribution', {}).get('out_of_stock', {}).get('count', 0)} | 0 VNƒê | {inventory_analysis.get('stock_distribution', {}).get('out_of_stock', {}).get('count', 0)/overview.get('total_products', 1)*100:.1f}% |

### üí∞ Gi√° tr·ªã t·ªìn kho:
- **T·ªïng gi√° tr·ªã**: {overview.get('total_inventory_value', 0):,.0f} VNƒê
- **T·ª∑ l·ªá quay v√≤ng**: {overview.get('inventory_turnover_ratio', 0):.2f} (l·∫ßn/nƒÉm)
- **V·ªën ƒë√≥ng bƒÉng** (h√†ng t·ªìn l√¢u): {inventory_analysis.get('stock_distribution', {}).get('well_stocked', {}).get('value', 0):,.0f} VNƒê
- **Kh·∫£ nƒÉng thanh kho·∫£n**: {'Cao' if overview.get('inventory_turnover_ratio', 0) > 4 else 'Trung b√¨nh' if overview.get('inventory_turnover_ratio', 0) > 2 else 'Th·∫•p'}

## 2Ô∏è‚É£ ∆ØU TI√äN NH·∫¨P H√ÄNG NGAY ‚ö°
T·∫°o b·∫£ng markdown:
| STT | S·∫£n ph·∫©m | T·ªìn hi·ªán t·∫°i | B√°n TB/ng√†y | H·∫øt sau X ng√†y | SL ƒë·ªÅ xu·∫•t nh·∫≠p |
|-----|----------|--------------|-------------|----------------|-----------------|

### üìã K·∫ø ho·∫°ch nh·∫≠p h√†ng chi ti·∫øt:
**TU·∫¶N N√ÄY (URGENT - T·ªìn kho 1-5):**
{json.dumps([{'t√™n': p.get('name'), 't·ªìn_kho': p.get('available_stock', 0), 'gi√°': f"{p.get('price', 0):,.0f} VNƒê"} for p in inventory_analysis.get('critical_stock_products', [])], indent=2, ensure_ascii=False)}
- T·ªïng v·ªën c·∫ßn: {sum([p.get('price', 0) * max(50 - p.get('available_stock', 0), 0) for p in inventory_analysis.get('critical_stock_products', [])]):,.0f} VNƒê

**TH√ÅNG N√ÄY (T·ªìn kho 6-15):**
{json.dumps([{'t√™n': p.get('name'), 't·ªìn_kho': p.get('available_stock', 0), 'gi√°': f"{p.get('price', 0):,.0f} VNƒê"} for p in inventory_analysis.get('warning_stock_products', [])], indent=2, ensure_ascii=False)}
- Ng√¢n s√°ch: {sum([p.get('price', 0) * max(30 - p.get('available_stock', 0), 0) for p in inventory_analysis.get('warning_stock_products', [])]):,.0f} VNƒê

## 3Ô∏è‚É£ X·ª¨ L√ù H√ÄNG T·ªíN KHO L√ÇU üóëÔ∏è
T·∫°o b·∫£ng markdown:
| S·∫£n ph·∫©m | T·ªìn | Gi√° tr·ªã | Th·ªùi gian t·ªìn | Gi·∫£i ph√°p ƒë·ªÅ xu·∫•t |
|----------|-----|---------|---------------|-------------------|

### Chi·∫øn l∆∞·ª£c x·ª≠ l√Ω:
1. **Flash Sale Weekend**: Gi·∫£m 40-50% cho top [X] s·∫£n ph·∫©m
2. **Bundle Deal**: K·∫øt h·ª£p v·ªõi s·∫£n ph·∫©m hot
3. **Clearance Sale**: X·ª≠ l√Ω t·ªìn kho c≈© v·ªõi gi·∫£m gi√° s√¢u
4. **Trade-in Program**: Thu c≈© ƒë·ªïi m·ªõi

## 4Ô∏è‚É£ CHI·∫æN L∆Ø·ª¢C T·ªêI ∆ØU T·ªíN KHO üéØ
### A. Ph√¢n lo·∫°i ABC:
- **Nh√≥m A** (20% SP, 80% gi√° tr·ªã): [Li·ªát k√™ s·∫£n ph·∫©m chi·∫øn l∆∞·ª£c]
- **Nh√≥m B** (30% SP, 15% gi√° tr·ªã): [S·∫£n ph·∫©m quan tr·ªçng]
- **Nh√≥m C** (50% SP, 5% gi√° tr·ªã): [S·∫£n ph·∫©m ph·ª•]

### B. C·∫£i thi·ªán v·∫≠n h√†nh:
1. **H·ªá th·ªëng qu·∫£n l√Ω kho:**
   - ƒê·ªÅ xu·∫•t ph·∫ßn m·ªÅm/c√¥ng c·ª• ph√π h·ª£p
   - Barcode/QR scanning
   
2. **Quy tr√¨nh ki·ªÉm k√™:**
   - T·∫ßn su·∫•t: [H√†ng tu·∫ßn/th√°ng]
   - Ph∆∞∆°ng ph√°p: [Cycle counting/Full inventory]
   
3. **S·∫Øp x·∫øp kho:**
   - Layout t·ªëi ∆∞u theo ABC
   - FIFO/LIFO strategy

### C. Ch√≠nh s√°ch an to√†n kho:
- **Safety Stock**: [X] ƒë∆°n v·ªã
- **Reorder Point**: Khi t·ªìn <= [Y]
- **Lead Time**: [Z] ng√†y
- **EOQ** (Economic Order Quantity): [T√≠nh to√°n]

## 5Ô∏è‚É£ K·∫æ HO·∫†CH D·ª∞ TR√ô 3 TH√ÅNG T·ªöI üìÖ
### Th√°ng 1 (Hi·ªán t·∫°i):
- Ng√¢n s√°ch: [...] VNƒê
- Danh m·ª•c ∆∞u ti√™n: [...]
- S·∫£n ph·∫©m c·∫ßn ƒë·∫©y m·∫°nh: [...]

### Th√°ng 2:
- M√πa v·ª•/s·ª± ki·ªán: [...]
- S·∫£n ph·∫©m seasonal: [...]

### Th√°ng 3:
- Chu·∫©n b·ªã cho: [...]
- S·∫£n ph·∫©m m·ªõi launch: [...]

## 6Ô∏è‚É£ CH·ªà S·ªê HI·ªÜU SU·∫§T KHO
T√≠nh to√°n v√† ƒë√°nh gi√°:
- **Inventory Turnover Ratio**: [...] l·∫ßn/nƒÉm [T·ªët/TB/C·∫ßn c·∫£i thi·ªán]
- **Days Sales of Inventory (DSI)**: [...] ng√†y
- **Stockout Rate**: [...]% [M·ª•c ti√™u: <5%]
- **Carrying Cost**: [...] VNƒê/th√°ng
- **Fill Rate**: [...]% [M·ª•c ti√™u: >95%]

‚ö° Ph√¢n t√≠ch chi ti·∫øt v·ªõi s·ªë li·ªáu c·ª• th·ªÉ, k·∫ø ho·∫°ch th·ª±c thi r√µ r√†ng!
"""

    elif analysis_type == 'sales':
        prompt = base_context + """

üöÄ NHI·ªÜM V·ª§: CHI·∫æN L∆Ø·ª¢C TƒÇNG TR∆Ø·ªûNG B√ÅN H√ÄNG & REVENUE OPTIMIZATION

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

üìã C·∫§U TR√öC B√ÅO C√ÅO Y√äU C·∫¶U:

## üìä EXECUTIVE SUMMARY - T√åNH H√åNH B√ÅN H√ÄNG
> T√≥m t·∫Øt 3-4 c√¢u v·ªÅ hi·ªán tr·∫°ng doanh s·ªë, highlight 2-3 insights quan tr·ªçng nh·∫•t v√† c∆° h·ªôi tƒÉng tr∆∞·ªüng l·ªõn nh·∫•t.

---

## üìà SALES PERFORMANCE DASHBOARD

### B·∫£ng ch·ªâ s·ªë b√°n h√†ng ch√≠nh:

| Ch·ªâ s·ªë | Gi√° tr·ªã hi·ªán t·∫°i | Benchmark | Gap | C∆° h·ªôi tƒÉng tr∆∞·ªüng |
|--------|------------------|-----------|-----|---------------------|
| üí∞ Doanh thu/th√°ng | [X] VNƒê | [Y] VNƒê | [Z]% | +[W]% n·∫øu ƒë·∫°t benchmark |
| üõí S·ªë ƒë∆°n h√†ng | [X] ƒë∆°n | [Y] ƒë∆°n | [Z]% | +[W] ƒë∆°n/th√°ng |
| üíµ AOV (Gi√° tr·ªã TB/ƒë∆°n) | [X] VNƒê | [Y] VNƒê | [Z]% | +[W] VNƒê/ƒë∆°n |
| üìä Conversion Rate | [X]% | [Y]% | [Z]% | +[W]% conversion |
| üîÑ Repeat Purchase Rate | [X]% | [Y]% | [Z]% | +[W]% retention |
| üë• Customer Lifetime Value | [X] VNƒê | [Y] VNƒê | [Z]% | +[W] VNƒê/kh√°ch |

**T·ªïng ti·ªÅm nƒÉng tƒÉng tr∆∞·ªüng**: +[X]% doanh thu n·∫øu ƒë·∫°t t·∫•t c·∫£ benchmarks

---

## üéØ PH√ÇN T√çCH SALES FUNNEL CHI TI·∫æT

### Conversion Funnel Analysis:

```
üëÅÔ∏è Traffic (100%)
    ‚Üì [-X]% drop
üõçÔ∏è Product View ([Y]%)
    ‚Üì [-X]% drop  ‚Üê ƒêI·ªÇM Y·∫æU 1: C·∫£i thi·ªán product pages
üõí Add to Cart ([Y]%)
    ‚Üì [-X]% drop  ‚Üê ƒêI·ªÇM Y·∫æU 2: Cart abandonment cao
üí≥ Checkout ([Y]%)
    ‚Üì [-X]% drop  ‚Üê ƒêI·ªÇM Y·∫æU 3: Friction trong thanh to√°n
‚úÖ Purchase ([Y]%)
```

### B·∫£ng ph√¢n t√≠ch t·ª´ng giai ƒëo·∫°n:

| Giai ƒëo·∫°n | Conversion | Benchmark | V·∫•n ƒë·ªÅ | Gi·∫£i ph√°p | Impact d·ª± ki·∫øn |
|-----------|------------|-----------|--------|-----------|----------------|
| View ‚Üí Cart | [X]% | [Y]% | [...] | [...] | +[Z]% orders |
| Cart ‚Üí Checkout | [X]% | [Y]% | [...] | [...] | +[Z]% orders |
| Checkout ‚Üí Purchase | [X]% | [Y]% | [...] | [...] | +[Z]% orders |

---

## üë• PH√ÇN KH√öC KH√ÅCH H√ÄNG & CHI·∫æN L∆Ø·ª¢C

### Customer Segmentation Matrix:

| Ph√¢n kh√∫c | % Kh√°ch h√†ng | % Doanh thu | AOV | Frequency | ƒê·∫∑c ƒëi·ªÉm | Chi·∫øn l∆∞·ª£c |
|-----------|--------------|-------------|-----|-----------|----------|------------|
| üíé VIP (High Value) | [X]% | [Y]% | [Z] VNƒê | [W] l·∫ßn/th√°ng | [...] | [...] |
| ‚≠ê Loyal (Regular) | [X]% | [Y]% | [Z] VNƒê | [W] l·∫ßn/th√°ng | [...] | [...] |
| üå± New (First-time) | [X]% | [Y]% | [Z] VNƒê | [W] l·∫ßn | [...] | [...] |
| üò¥ At-Risk (Churning) | [X]% | [Y]% | [Z] VNƒê | [W] l·∫ßn | [...] | [...] |
| üíî Lost (Inactive) | [X]% | [Y]% | [Z] VNƒê | 0 | [...] | [...] |

### Chi·∫øn l∆∞·ª£c cho t·ª´ng ph√¢n kh√∫c:

#### üíé VIP Customers (Protect & Grow)
1. **VIP Loyalty Program**:
   - Exclusive perks: Early access, special pricing
   - Personal account manager
   - Birthday/anniversary gifts
   - **Target**: TƒÉng AOV +20%, Frequency +30%

2. **Upsell/Cross-sell Premium**:
   - Premium product recommendations
   - Bundle deals exclusive for VIP
   - **Expected**: +[X] VNƒê/kh√°ch/th√°ng

#### ‚≠ê Loyal Customers (Maximize Value)
1. **Referral Program**: Th∆∞·ªüng [X] VNƒê cho m·ªói gi·ªõi thi·ªáu th√†nh c√¥ng
2. **Subscription Model**: Gi·∫£m [Y]% cho ƒëƒÉng k√Ω ƒë·ªãnh k·ª≥
3. **Target**: Chuy·ªÉn [Z]% l√™n VIP tier

#### üå± New Customers (Convert & Retain)
1. **Welcome Journey** (7 ng√†y):
   - Day 0: Welcome email + 10% off next purchase
   - Day 2: Product education + use cases
   - Day 5: Social proof + reviews
   - Day 7: Urgency + limited offer
2. **First Purchase Incentive**: Free shipping + gift
3. **Target**: [X]% repeat purchase trong 30 ng√†y

#### üò¥ At-Risk Customers (Win-back)
1. **Re-engagement Campaign**:
   - "We miss you" email v·ªõi 15% discount
   - Survey: T·∫°i sao kh√¥ng mua n·ªØa?
   - Personalized offers d·ª±a tr√™n l·ªãch s·ª≠
2. **Target**: Win-back [X]% trong 60 ng√†y

#### üíî Lost Customers (Reactivation)
1. **Win-back Campaign**: 20-30% discount + free shipping
2. **New product announcement**: "Look what's new"
3. **Target**: Reactivate [X]% trong 90 ng√†y

---

## üéØ CHI·∫æN L∆Ø·ª¢C TƒÇNG AOV (Average Order Value)

### M·ª•c ti√™u: TƒÉng AOV t·ª´ [X] VNƒê l√™n [Y] VNƒê (+[Z]%)

#### A. Product Bundling Strategy

üö® **QUY T·∫ÆC NGHI√äM NG·∫∂T: CH·ªà S·ª¨ D·ª§NG S·∫¢N PH·∫®M C√ì TRONG D·ªÆ LI·ªÜU**

**KI·ªÇM TRA TR∆Ø·ªöC KHI ƒê·ªÄ XU·∫§T:**
1. ‚úÖ S·∫£n ph·∫©m c√≥ trong TOP 5 s·∫£n ph·∫©m n·ªïi b·∫≠t?
2. ‚úÖ S·∫£n ph·∫©m c√≥ trong danh m·ª•c ƒë√£ cung c·∫•p?
3. ‚úÖ Kh√¥ng ph·∫£i 2 s·∫£n ph·∫©m c√πng lo·∫°i (2 laptop, 2 ƒëi·ªán tho·∫°i)?
4. ‚úÖ Logic b·ªï sung/h·ªó tr·ª£ h·ª£p l√Ω?

**N·∫æU KH√îNG ƒê·ª¶ ƒêI·ªÄU KI·ªÜN ‚Üí GHI:**
"‚ö†Ô∏è **Kh√¥ng th·ªÉ ƒë·ªÅ xu·∫•t combo**: 
- L√Ω do: D·ªØ li·ªáu hi·ªán t·∫°i kh√¥ng c√≥ s·∫£n ph·∫©m ph·ª• ki·ªán/b·ªï sung
- Khuy·∫øn ngh·ªã: Nh·∫≠p th√™m ph·ª• ki·ªán (chu·ªôt, balo, ·ªëp l∆∞ng, tai nghe, s·∫°c d·ª± ph√≤ng...) ƒë·ªÉ t·∫°o combo tƒÉng AOV"

**N·∫æU ƒê·ª¶ ƒêI·ªÄU KI·ªÜN ‚Üí T·∫†O B·∫¢NG:**

| Bundle Name | S·∫£n ph·∫©m (T·ª™ DATA) | Gi√° l·∫ª | Gi√° bundle | Ti·∫øt ki·ªám | Logic |
|-------------|---------------------|--------|------------|-----------|-------|
| [T√™n combo] | [SP A - t√™n ch√≠nh x√°c] + [SP B - t√™n ch√≠nh x√°c] | [X] VNƒê | [Y] VNƒê | [Z]% | [T·∫°i sao kh√°ch c·∫ßn combo n√†y] |

**S·ªë l∆∞·ª£ng combo:** T·ªëi ƒëa 3-5 combo - D·ª∞A HO√ÄN TO√ÄN V√ÄO D·ªÆ LI·ªÜU C√ì S·∫¥N

#### B. Upselling Tactics
1. **Product Page Upsells**:
   - "Customers also bought" section
   - "Upgrade to premium version" v·ªõi so s√°nh r√µ r√†ng
   - Limited-time upgrade offers

2. **Cart Upsells**:
   - "Add [Product X] for only [Y] VNƒê more"
   - Free shipping threshold: "Th√™m [X] VNƒê ƒë·ªÉ ƒë∆∞·ª£c free ship"
   - Volume discounts: "Mua 2 gi·∫£m 10%, mua 3 gi·∫£m 15%"

#### C. Cross-selling Strategy
1. **Intelligent Recommendations**:
   - AI-powered "You may also like"
   - "Complete the look/set"
   - Accessories & add-ons

2. **Post-purchase Cross-sell**:
   - Thank you page offers
   - Follow-up emails v·ªõi related products

**Expected Impact**: TƒÉng AOV +[X]% = +[Y] VNƒê doanh thu/th√°ng

---

## üì¢ MULTI-CHANNEL MARKETING PLAYBOOK

### A. PAID ADVERTISING STRATEGY

#### 1. Facebook & Instagram Ads

| Campaign Type | Budget/th√°ng | Target Audience | Objective | Expected ROAS |
|---------------|--------------|-----------------|-----------|---------------|
| Prospecting | [X] VNƒê | Lookalike 1-3% | Acquisition | 3-4X |
| Retargeting - Cart | [X] VNƒê | Cart abandoners | Conversion | 5-7X |
| Retargeting - View | [X] VNƒê | Product viewers | Conversion | 4-5X |
| Engagement | [X] VNƒê | Page engagers | Awareness | 2-3X |

**Creative Strategy**:
- Video ads: Product demos, testimonials
- Carousel ads: Showcase bundles
- Collection ads: Category browsing
- Stories ads: Limited-time offers

#### 2. Google Ads Strategy

| Campaign Type | Budget/th√°ng | Keywords | Expected CTR | Expected ROAS |
|---------------|--------------|----------|--------------|---------------|
| Search - Brand | [X] VNƒê | Brand terms | [Y]% | 8-10X |
| Search - Generic | [X] VNƒê | Product terms | [Y]% | 4-5X |
| Shopping | [X] VNƒê | Product feed | [Y]% | 5-6X |
| Display Remarketing | [X] VNƒê | Site visitors | [Y]% | 3-4X |

#### 3. TikTok Ads (if applicable)
- Spark Ads v·ªõi UGC content
- In-Feed Ads v·ªõi trending sounds
- Budget: [X] VNƒê/th√°ng
- Target ROAS: 3-5X

**Total Marketing Budget**: [X] VNƒê/th√°ng
**Expected Revenue**: [Y] VNƒê/th√°ng
**Overall ROAS Target**: 4-5X

### B. ORGANIC MARKETING STRATEGY

#### 1. Content Marketing Calendar

| Week | Content Type | Topic | Platform | Goal |
|------|--------------|-------|----------|------|
| 1 | Blog post | [Topic] | Website | SEO traffic |
| 1 | Video | Product review | YouTube | Education |
| 1 | Infographic | [Topic] | Social | Engagement |
| 2 | ... | ... | ... | ... |

#### 2. Social Media Strategy
- **Facebook**: 5-7 posts/tu·∫ßn (mix: 40% educational, 30% promotional, 30% engagement)
- **Instagram**: Daily posts + 3-5 Stories/ng√†y
- **TikTok**: 3-5 videos/tu·∫ßn (trending challenges, product demos)
- **Target**: TƒÉng followers +50%, engagement rate >5%

#### 3. Email Marketing Automation

**Flows c·∫ßn setup:**

1. **Welcome Series** (5 emails, 10 ng√†y):
   - Email 1 (Day 0): Welcome + 10% discount code
   - Email 2 (Day 2): Brand story + bestsellers
   - Email 3 (Day 5): Educational content + use cases
   - Email 4 (Day 7): Social proof + reviews
   - Email 5 (Day 10): Last chance + urgency

2. **Abandoned Cart Recovery** (3 emails):
   - Email 1 (1 gi·ªù): Gentle reminder
   - Email 2 (24 gi·ªù): 5% discount incentive
   - Email 3 (48 gi·ªù): 10% discount + free shipping

3. **Post-Purchase** (4 emails):
   - Email 1 (Ngay sau): Thank you + tracking
   - Email 2 (3 ng√†y): How to use + tips
   - Email 3 (7 ng√†y): Review request + incentive
   - Email 4 (14 ng√†y): Cross-sell recommendations

4. **Win-back Campaign** (Inactive 60+ ng√†y):
   - Email 1: "We miss you" + 15% off
   - Email 2: New arrivals showcase
   - Email 3: Last chance + 20% off

**Expected Email Performance**:
- Open rate: 25-30%
- Click rate: 3-5%
- Conversion rate: 2-3%
- Revenue from email: [X]% of total

---

## üéÅ PROMOTIONAL CALENDAR & CAMPAIGNS

### Quarterly Promotion Strategy:

| Th√°ng | Campaign | Discount | Duration | Products | Budget | Expected Revenue |
|-------|----------|----------|----------|----------|--------|------------------|
| 1 | New Year Sale | 20-30% | 7 ng√†y | All | [X] VNƒê | [Y] VNƒê |
| 1 | Flash Sale Friday | 40% | 24h | Selected | [X] VNƒê | [Y] VNƒê |
| 2 | Valentine's Day | 15% + Gift | 3 ng√†y | Bundles | [X] VNƒê | [Y] VNƒê |
| 2 | Mid-month Madness | BOGO 50% | 48h | Slow movers | [X] VNƒê | [Y] VNƒê |
| 3 | Spring Collection | 10% | 14 ng√†y | New arrivals | [X] VNƒê | [Y] VNƒê |
| 3 | Clearance Sale | 50-70% | 7 ng√†y | Old stock | [X] VNƒê | [Y] VNƒê |

### Loyalty & Referral Programs:

**Loyalty Program Design**:
- Earn 1 point per 1,000 VNƒê spent
- Tiers: Bronze (0-999), Silver (1000-4999), Gold (5000+)
- Benefits per tier: [Li·ªát k√™ c·ª• th·ªÉ]
- Expected participation: [X]% customers

**Referral Program**:
- Referrer gets: [X] VNƒê credit
- Referee gets: [Y]% off first order
- Target: [Z] referrals/th√°ng

---

## üöÄ CONVERSION RATE OPTIMIZATION (CRO)

### A. Website Optimization Checklist

#### Homepage:
- [ ] Clear value proposition above the fold
- [ ] Featured products/bestsellers prominently displayed
- [ ] Trust signals: Reviews, ratings, badges
- [ ] Mobile-optimized (>50% traffic l√† mobile)
- [ ] Page load time <3 seconds

#### Product Pages:
- [ ] High-quality images (5-7 photos + video)
- [ ] Detailed descriptions with benefits (not just features)
- [ ] Customer reviews & ratings visible
- [ ] Clear CTA button (contrasting color)
- [ ] Stock urgency ("Only X left!")
- [ ] Social proof ("Y people viewing this")
- [ ] Size guide/comparison chart
- [ ] Related products section

#### Cart & Checkout:
- [ ] Progress indicator (4 steps ‚Üí 1 page checkout)
- [ ] Guest checkout option
- [ ] Multiple payment methods (COD, card, e-wallet)
- [ ] Trust badges (SSL, secure payment)
- [ ] Free shipping threshold visible
- [ ] Exit-intent popup (cart abandonment)
- [ ] Save cart for later
- [ ] Mobile-optimized checkout

### B. A/B Testing Roadmap

| Test | Variant A | Variant B | Metric | Expected Lift |
|------|-----------|-----------|--------|---------------|
| CTA Button | "Mua ngay" | "Th√™m v√†o gi·ªè" | CTR | +5-10% |
| Product Image | Lifestyle | White background | Conversion | +3-5% |
| Pricing Display | 999,000ƒë | 999.000ƒë | Conversion | +2-3% |
| Checkout Flow | Multi-step | One-page | Completion | +10-15% |

---

## üìä GROWTH ROADMAP - TƒÇNG TR∆Ø·ªûNG 50% TRONG 6 TH√ÅNG

### üéØ Phase 1: TH√ÅNG 1-2 (Foundation) - Target: +15% Revenue

#### Quick Wins (Tu·∫ßn 1-2):
1. **Setup Email Automation** (Impact: +5% revenue)
   - Abandoned cart recovery
   - Welcome series
   - Post-purchase flow
   - **Budget**: 0 VNƒê (s·ª≠ d·ª•ng tools c√≥ s·∫µn)
   - **Timeline**: 1 tu·∫ßn

2. **Optimize Top 10 Product Pages** (Impact: +3% conversion)
   - Add more images & videos
   - Improve descriptions
   - Add reviews
   - **Budget**: [X] VNƒê (photography)
   - **Timeline**: 1 tu·∫ßn

3. **Launch First Bundle Offers** (Impact: +10% AOV)
   - Create 3-5 bundles
   - Promote on homepage
   - **Budget**: 0 VNƒê
   - **Timeline**: 3 ng√†y

#### Growth Initiatives (Tu·∫ßn 3-8):
4. **Facebook Ads Campaign** (Impact: +20% traffic)
   - Prospecting + Retargeting
   - **Budget**: [X] VNƒê/th√°ng
   - **Expected ROAS**: 4X
   - **Timeline**: Ongoing

5. **Loyalty Program Launch** (Impact: +8% repeat rate)
   - Design tier structure
   - Integrate with website
   - **Budget**: [Y] VNƒê (setup)
   - **Timeline**: 2 tu·∫ßn

**Phase 1 KPIs**:
- Revenue: +15% ([X] VNƒê ‚Üí [Y] VNƒê)
- Orders: +12%
- AOV: +10%
- Conversion: +3%

### üöÄ Phase 2: TH√ÅNG 3-4 (Acceleration) - Target: +20% Revenue

#### Initiatives:
6. **Google Ads Expansion** (Impact: +15% traffic)
7. **Referral Program Launch** (Impact: +10% new customers)
8. **Content Marketing** (Impact: +20% organic traffic)
9. **Influencer Partnerships** (Impact: +25% brand awareness)
10. **One-page Checkout** (Impact: +12% checkout conversion)

**Phase 2 KPIs**:
- Revenue: +20% cumulative
- New customers: +30%
- Organic traffic: +40%

### üéØ Phase 3: TH√ÅNG 5-6 (Scale & Optimize) - Target: +15% Revenue

#### Initiatives:
11. **TikTok Ads** (Impact: +20% younger audience)
12. **Advanced Segmentation** (Impact: +15% email revenue)
13. **Subscription Model** (Impact: +25% predictable revenue)
14. **Mobile App** (Impact: +30% retention)
15. **Marketplace Expansion** (Shopee, Lazada, Tiki)

**Phase 3 KPIs**:
- Revenue: +50% cumulative (vs th√°ng 0)
- Customer base: +60%
- Repeat rate: +40%

---

## üìä KPI DASHBOARD & TRACKING

### Weekly Tracking:
1. **Revenue**: [X] VNƒê (Target: [Y] VNƒê)
2. **Orders**: [X] ƒë∆°n (Target: [Y] ƒë∆°n)
3. **AOV**: [X] VNƒê (Target: [Y] VNƒê)
4. **Conversion Rate**: [X]% (Target: [Y]%)
5. **Traffic**: [X] visitors (Target: [Y] visitors)

### Monthly Tracking:
1. **Revenue Growth MoM**: [X]% (Target: +8-10%/th√°ng)
2. **Customer Acquisition**: [X] kh√°ch m·ªõi (Target: [Y])
3. **CAC (Customer Acquisition Cost)**: [X] VNƒê (Target: <[Y] VNƒê)
4. **LTV (Lifetime Value)**: [X] VNƒê (Target: >[Y] VNƒê)
5. **LTV:CAC Ratio**: [X]:1 (Target: >3:1)
6. **Repeat Purchase Rate**: [X]% (Target: [Y]%)
7. **Email Revenue %**: [X]% (Target: 20-30%)
8. **Paid Ads ROAS**: [X]X (Target: >4X)

### Quarterly Goals:
- **Revenue**: +[X]% vs qu√Ω tr∆∞·ªõc
- **Profit Margin**: [Y]% (Target: [Z]%)
- **Market Share**: [X]% (Target: +[Y]%)
- **Customer Satisfaction**: [X]% (Target: >90%)

---

## üí° K·∫æT LU·∫¨N & H√ÄNH ƒê·ªòNG ∆ØU TI√äN

### üéØ Top 5 Priorities (L√†m ngay tu·∫ßn n√†y):
1. **[Action 1]**: [M√¥ t·∫£ + Expected impact]
2. **[Action 2]**: [M√¥ t·∫£ + Expected impact]
3. **[Action 3]**: [M√¥ t·∫£ + Expected impact]
4. **[Action 4]**: [M√¥ t·∫£ + Expected impact]
5. **[Action 5]**: [M√¥ t·∫£ + Expected impact]

### üìà Revenue Forecast (6 th√°ng):
- **Th√°ng 1-2**: [X] VNƒê (+15%)
- **Th√°ng 3-4**: [Y] VNƒê (+35% cumulative)
- **Th√°ng 5-6**: [Z] VNƒê (+50% cumulative)
- **Total Additional Revenue**: +[W] VNƒê

### üí∞ Investment Required:
- Marketing: [X] VNƒê
- Technology: [Y] VNƒê
- Content: [Z] VNƒê
- **Total**: [W] VNƒê
- **Expected ROI**: [V]X

### ‚ö†Ô∏è Risk Mitigation:
1. **Risk**: [M√¥ t·∫£] ‚Üí **Mitigation**: [Gi·∫£i ph√°p]
2. **Risk**: [M√¥ t·∫£] ‚Üí **Mitigation**: [Gi·∫£i ph√°p]

---

‚ö° **Y√äU C·∫¶U FORMAT:**
- S·ªë li·ªáu C·ª§ TH·ªÇ v·ªõi ƒë∆°n v·ªã VNƒê, %, timeline r√µ r√†ng
- M·ªói chi·∫øn l∆∞·ª£c c√≥: M·ª•c ti√™u + C√°ch l√†m + Budget + Timeline + KPI + Expected ROI
- ∆Øu ti√™n ACTIONABLE tactics c√≥ th·ªÉ tri·ªÉn khai ngay
- ƒê·ªô d√†i: 1500-2000 t·ª´
- Vi·∫øt ti·∫øng Vi·ªát chuy√™n nghi·ªáp, d·ªÖ hi·ªÉu, c√≥ c·∫•u tr√∫c
"""

    else:
        prompt = base_context + "\n\nPh√¢n t√≠ch t·ªïng quan v√† ƒë∆∞a ra ƒë·ªÅ xu·∫•t."

    return prompt


@router.get("/chroma-data")
async def get_all_chroma_data():
    """
    Endpoint ƒë·ªÉ hi·ªÉn th·ªã t·∫•t c·∫£ d·ªØ li·ªáu ƒë∆∞·ª£c l∆∞u trong Chroma DB instance chroma_analytics
    
    Returns:
        Dict ch·ª©a t·∫•t c·∫£ collections v√† d·ªØ li·ªáu c·ªßa ch√∫ng
    """
    try:
        global chroma_client
        if chroma_client is None:
            return {"error": "ChromaDB client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o"}
        
        # L·∫•y t·∫•t c·∫£ collections
        collections = chroma_client.list_collections()
        
        result = {
            "instance_path": "./chroma_analytics",
            "total_collections": len(collections),
            "collections": {},
            "timestamp": datetime.now().isoformat()
        }
        
        # Duy·ªát qua t·ª´ng collection
        for collection in collections:
            collection_name = collection.name
            
            try:
                # L·∫•y t·∫•t c·∫£ documents - kh√¥ng c·∫ßn include v√¨ m·∫∑c ƒë·ªãnh ƒë√£ c√≥ ids, documents, metadatas
                all_data = collection.get()
                
                result["collections"][collection_name] = {
                    "metadata": collection.metadata,
                    "total_documents": len(all_data.get('ids', [])),
                    "documents": []
                }
                
                # T·∫°o danh s√°ch documents v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin
                ids = all_data.get('ids', [])
                documents = all_data.get('documents', [])
                metadatas = all_data.get('metadatas', [])
                
                for i, doc_id in enumerate(ids):
                    doc_info = {
                        "id": doc_id,
                        "content": documents[i] if i < len(documents) else None,
                        "metadata": metadatas[i] if metadatas and i < len(metadatas) else None
                    }
                    result["collections"][collection_name]["documents"].append(doc_info)
                    
            except Exception as e:
                result["collections"][collection_name] = {
                    "error": f"Kh√¥ng th·ªÉ ƒë·ªçc collection: {str(e)}",
                    "metadata": collection.metadata
                }
        
        print(f"[Chroma Data] Retrieved data from {len(collections)} collections")
        return result
        
    except Exception as e:
        return {"error": f"L·ªói khi truy c·∫≠p Chroma DB: {str(e)}"}


@router.get("/chroma-stats")
async def get_chroma_stats():
    """
    Endpoint ƒë·ªÉ l·∫•y th·ªëng k√™ nhanh v·ªÅ Chroma DB
    
    Returns:
        Dict ch·ª©a th·ªëng k√™ t·ªïng quan
    """
    try:
        global chroma_client
        if chroma_client is None:
            return {"error": "ChromaDB client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o"}
        
        collections = chroma_client.list_collections()
        
        stats = {
            "instance_path": "./chroma_analytics",
            "total_collections": len(collections),
            "collections_stats": {},
            "total_documents": 0,
            "timestamp": datetime.now().isoformat()
        }
        
        for collection in collections:
            try:
                count = collection.count()
                stats["collections_stats"][collection.name] = {
                    "documents_count": count,
                    "metadata": collection.metadata
                }
                stats["total_documents"] += count
            except Exception as e:
                stats["collections_stats"][collection.name] = {
                    "error": str(e),
                    "metadata": collection.metadata
                }
        
        return stats
        
    except Exception as e:
        return {"error": f"L·ªói khi l·∫•y th·ªëng k√™ Chroma DB: {str(e)}"}


class SyncDataRequest(BaseModel):
    """Request model for data synchronization"""
    spring_service_url: Optional[str] = None
    auth_token: str
    clear_existing: Optional[bool] = True


class ProcessDocumentRequest(BaseModel):
    """Request model for document processing"""
    file_path: str
    business_id: str
    business_username: str
    file_name: str
    file_type: str
    description: Optional[str] = None


@router.post("/process-document")
async def process_business_document(request: ProcessDocumentRequest):
    """
    X·ª≠ l√Ω t√†i li·ªáu doanh nghi·ªáp v√† l∆∞u v√†o ChromaDB collection ri√™ng

    Args:
        request: Th√¥ng tin t√†i li·ªáu c·∫ßn x·ª≠ l√Ω

    Returns:
        Dict ch·ª©a k·∫øt qu·∫£ x·ª≠ l√Ω
    """
    try:
        global chroma_client
        if chroma_client is None:
            raise HTTPException(status_code=500, detail="ChromaDB client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")

        # Kh·ªüi t·∫°o document processor
        doc_processor = get_document_processor()

        # Extract text content t·ª´ file
        print(f"[Document Processing] Processing file: {request.file_path}")
        extracted_text, metadata = doc_processor.extract_text_from_file(
            request.file_path,
            request.file_type
        )

        if not metadata.get("extraction_success", False):
            raise HTTPException(
                status_code=400,
                detail=f"Kh√¥ng th·ªÉ x·ª≠ l√Ω t√†i li·ªáu: {metadata.get('error', 'Unknown error')}"
            )

        # Chu·∫©n b·ªã metadata cho ChromaDB
        doc_metadata = {
            "data_type": "document",
            "document_id": f"doc_{request.business_id}_{int(datetime.now().timestamp())}",
            "business_id": request.business_id,
            "business_username": request.business_username,
            "file_name": request.file_name,
            "file_type": request.file_type,
            "file_path": request.file_path,
            "description": request.description or "",
            "processed_at": datetime.now().isoformat(),
            "content_length": metadata.get("content_length", 0),
            "extraction_success": True
        }

        # Th√™m metadata t·ª´ qu√° tr√¨nh processing n·∫øu c√≥
        if "sheets" in metadata:
            doc_metadata["excel_sheets"] = json.dumps(metadata["sheets"])
        if "columns" in metadata:
            doc_metadata["csv_columns"] = metadata["columns"]
        if "rows" in metadata:
            doc_metadata["data_rows"] = metadata["rows"]

        # Validate and sanitize metadata
        sanitized_metadata = sanitize_metadata(doc_metadata)

        # T·∫°o content ƒë·∫ßy ƒë·ªß v·ªõi extracted text + metadata
        doc_content = f"""
DOCUMENT CONTENT:
{extracted_text}

---
METADATA:
Document ID: {doc_metadata["document_id"]}
Business: {request.business_username}
File Name: {request.file_name}
File Type: {request.file_type}
Description: {request.description or ""}
Processing Status: Success
Content Length: {len(extracted_text)} characters
Processed At: {doc_metadata["processed_at"]}
"""

        # L∆∞u v√†o documents collection ri√™ng bi·ªát
        analytics_rag_service = AnalyticsRAGService()
        result = analytics_rag_service.store_business_document(
            document_id=doc_metadata["document_id"],
            document_content=doc_content,
            metadata=sanitized_metadata
        )

        print(f"[Document Processing] Successfully processed and stored document: {doc_metadata['document_id']}")

        return {
            "success": True,
            "document_id": doc_metadata["document_id"],
            "content_length": len(extracted_text),
            "metadata": sanitized_metadata,
            "message": "T√†i li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† l∆∞u th√†nh c√¥ng"
        }

    except Exception as e:
        print(f"[Document Processing] Error: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"L·ªói x·ª≠ l√Ω t√†i li·ªáu: {str(e)}")


@router.post("/sync-from-spring")
async def sync_data_from_spring(request: SyncDataRequest):
    """
    ƒê·ªìng b·ªô d·ªØ li·ªáu t·ª´ Spring Service v√†o ChromaDB
    
    Args:
        request: Ch·ª©a URL Spring Service, token x√°c th·ª±c v√† option x√≥a d·ªØ li·ªáu c≈©
        
    Returns:
        Dict ch·ª©a k·∫øt qu·∫£ ƒë·ªìng b·ªô
    """
    try:
        global chroma_client
        if chroma_client is None:
            raise HTTPException(status_code=500, detail="ChromaDB client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o")
        
        # L·∫•y Spring Service URL t·ª´ bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c request
        spring_base_url = request.spring_service_url or os.getenv('SPRING_SERVICE_URL')
        if not spring_base_url:
            raise HTTPException(status_code=400, detail="SPRING_SERVICE_URL kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh")
        
        # Parse JWT token ƒë·ªÉ l·∫•y user info
        payload = parse_jwt_token(request.auth_token)
        if not payload:
            raise HTTPException(status_code=401, detail="Invalid JWT token")
        
        user_role = payload.get('role')
        user_id = payload.get('userId')
        
        # Determine endpoint based on user role
        if user_role == 'ADMIN':
            spring_url = f"{spring_base_url}/admin/analytics/system-data"
            print(f"[Sync] ADMIN user - fetching ALL system data from: {spring_url}")
        elif user_role == 'BUSINESS' and user_id:
            spring_url = f"{spring_base_url}/admin/analytics/business-data/{user_id}"
            print(f"[Sync] BUSINESS user (id={user_id}) - fetching filtered business data from: {spring_url}")
        else:
            raise HTTPException(
                status_code=403, 
                detail=f"User role '{user_role}' not authorized for analytics sync"
            )
        
        # L·∫•y d·ªØ li·ªáu t·ª´ Spring Service
        headers = {
            "Authorization": f"Bearer {request.auth_token}",
            "Content-Type": "application/json"
        }
        
        print(f"[Sync] Fetching data from: {spring_url}")
        
        response = requests.get(spring_url, headers=headers, timeout=30)
        
        if response.status_code != 200:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Failed to fetch data from Spring Service: {response.text}"
            )
        
        data = response.json()
        print(f"[Sync] Received data with {len(data.get('products', []))} products, {len(data.get('orders', []))} orders")
        
        sync_results = {
            "timestamp": datetime.now().isoformat(),
            "clear_existing": request.clear_existing,
            "products": {"total": 0, "with_details": 0, "success": 0, "errors": 0},
            "orders": {"total": 0, "success": 0, "errors": 0},
            "categories": {"total": 0, "success": 0, "errors": 0},
            "business_performance": {"total": 0, "success": 0, "errors": 0},
            "discounts": {"total": 0, "success": 0, "errors": 0},
            "users": {"total": 0, "success": 0, "errors": 0},
            "documents": {"total": 0, "success": 0, "errors": 0},
            "errors": []
        }
        
        # Kh·ªüi t·∫°o ho·∫∑c l·∫•y c√°c collections
        # Collection 1: business_data - ch·ª©a products, categories, business performance, discounts
        # Collection 2: orders_analytics - ch·ª©a orders
        # Collection 3: trends - ch·ª©a insights v√† trends (t∆∞∆°ng lai)
        # Collection 4: revenue_overview - ch·ª©a d·ªØ li·ªáu t·ªïng quan doanh thu v√† th·ªëng k√™ h·ªá th·ªëng
        # Collection 5: business_documents - ch·ª©a t√†i li·ªáu doanh nghi·ªáp ƒë√£ x·ª≠ l√Ω cho RAG
        
        if request.clear_existing:
            print("[Sync] Clearing existing data...")
            try:
                # X√≥a c√°c collections c≈©
                for collection_name in ["business_data", "orders_analytics", "trends", "revenue_overview", "business_documents"]:
                    try:
                        chroma_client.delete_collection(name=collection_name)
                        print(f"[Sync] Deleted old {collection_name} collection")
                    except:
                        pass
                
                # T·∫°o l·∫°i c√°c collections
                business_collection = chroma_client.create_collection(
                    name="business_data",
                    metadata={"description": "Products, categories, business performance, and discounts"}
                )
                orders_collection = chroma_client.create_collection(
                    name="orders_analytics",
                    metadata={"description": "Order data for analytics"}
                )
                trends_collection = chroma_client.create_collection(
                    name="trends",
                    metadata={"description": "Business trends and insights"}
                )
                revenue_collection = chroma_client.create_collection(
                    name="revenue_overview",
                    metadata={"description": "Revenue overview and system statistics"}
                )
                documents_collection = chroma_client.create_collection(
                    name="business_documents",
                    metadata={"description": "Business documents for RAG analysis"}
                )
                print("[Sync] Created new collections: business_data, orders_analytics, trends, revenue_overview, business_documents")
                
            except Exception as e:
                print(f"[Sync] Error clearing data: {e}")
                sync_results["errors"].append(f"Clear data error: {str(e)}")
                raise HTTPException(status_code=500, detail=f"Failed to clear collections: {str(e)}")
        else:
            # L·∫•y ho·∫∑c t·∫°o collections n·∫øu ch∆∞a c√≥
            print("[Sync] Getting or creating collections...")
            business_collection = chroma_client.get_or_create_collection(
                name="business_data",
                metadata={"description": "Products, categories, business performance, and discounts"}
            )
            orders_collection = chroma_client.get_or_create_collection(
                name="orders_analytics",
                metadata={"description": "Order data for analytics"}
            )
            trends_collection = chroma_client.get_or_create_collection(
                name="trends",
                metadata={"description": "Business trends and insights"}
            )
            revenue_collection = chroma_client.get_or_create_collection(
                name="revenue_overview",
                metadata={"description": "Revenue overview and system statistics"}
            )
            documents_collection = chroma_client.get_or_create_collection(
                name="business_documents",
                metadata={"description": "Business documents for RAG analysis"}
            )
            print("[Sync] Collections ready: business_data, orders_analytics, trends, revenue_overview, business_documents")
        
        # ƒê·ªìng b·ªô Products v·ªõi details ƒë·∫ßy ƒë·ªß
        if data.get('products'):
            sync_results["products"]["total"] = len(data['products'])
            print(f"[Sync] Syncing {len(data['products'])} products...")
            
            for product in data['products']:
                try:
                    product_id = str(product.get('id', ''))
                    has_details = bool(product.get('details'))
                    
                    if has_details:
                        sync_results["products"]["with_details"] += 1
                    
                    # T·∫°o product content v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin
                    product_content = f"""Product ID: {product.get('id')}
Name: {product.get('name', '')}
Description: {product.get('description', '')}
Price: {product.get('price', 0)} VND
Quantity: {product.get('quantity', 0)}
Status: {product.get('status', 'UNKNOWN')}
Category: {product.get('categoryName', '')}
Seller: {product.get('sellerUsername', '')}
"""
                    
                    # Parse details n·∫øu c√≥
                    details_text = ""
                    if product.get('details'):
                        try:
                            import json
                            details = json.loads(product['details']) if isinstance(product['details'], str) else product['details']
                            
                            if details:
                                details_text = "\nProduct Details:\n"
                                
                                # Basic details
                                if details.get('brand'):
                                    details_text += f"Brand: {details['brand']}\n"
                                if details.get('model'):
                                    details_text += f"Model: {details['model']}\n"
                                if details.get('color'):
                                    details_text += f"Color: {details['color']}\n"
                                if details.get('warranty'):
                                    details_text += f"Warranty: {details['warranty']}\n"
                                if details.get('storage'):
                                    details_text += f"Storage: {details['storage']}\n"
                                if details.get('type'):
                                    details_text += f"Type: {details['type']}\n"
                                
                                # Features
                                if details.get('features') and isinstance(details['features'], list):
                                    details_text += f"Features: {', '.join(details['features'])}\n"
                                
                                # Specifications
                                if details.get('specifications') and isinstance(details['specifications'], dict):
                                    details_text += "Specifications:\n"
                                    for key, value in details['specifications'].items():
                                        details_text += f"  {key}: {value}\n"
                                
                                # Connectivity
                                if details.get('connectivity') and isinstance(details['connectivity'], list):
                                    details_text += f"Connectivity: {', '.join(details['connectivity'])}\n"
                                
                                # Accessories
                                if details.get('accessories') and isinstance(details['accessories'], list):
                                    details_text += f"Accessories: {', '.join(details['accessories'])}\n"
                                
                                # Dimensions and Weight
                                if details.get('dimensions'):
                                    details_text += f"Dimensions: {details['dimensions']}\n"
                                if details.get('weight'):
                                    details_text += f"Weight: {details['weight']}\n"
                                
                                product_content += details_text
                                
                        except json.JSONDecodeError:
                            print(f"[Sync] Invalid JSON in product details for {product_id}")
                        except Exception as e:
                            print(f"[Sync] Error parsing product details for {product_id}: {e}")
                    
                    # Safe conversion cho metadata
                    price = product.get('price')
                    price_float = float(price) if price is not None else 0.0
                    
                    quantity = product.get('quantity')
                    quantity_int = int(quantity) if quantity is not None else 0
                    
                    total_sold = product.get('totalSold')
                    total_sold_int = int(total_sold) if total_sold is not None else 0
                    
                    total_revenue = product.get('totalRevenue')
                    total_revenue_float = float(total_revenue) if total_revenue is not None else 0.0
                    
                    # Prepare metadata v·ªõi ƒê·∫¶Y ƒê·ª¶ t·∫•t c·∫£ c√°c tr∆∞·ªùng t·ª´ DTO
                    product_metadata = {
                        "data_type": "product",
                        "product_id": product_id,
                        "name": product.get('name', ''),
                        "description": product.get('description', ''),
                        "category": product.get('categoryName', ''),
                        "category_id": str(product.get('categoryId', '')) if product.get('categoryId') else '',
                        "status": product.get('status', 'UNKNOWN'),
                        "price": price_float,
                        "quantity": quantity_int,
                        "seller": product.get('sellerUsername', ''),
                        "seller_id": str(product.get('sellerId', '')),
                        "total_sold": total_sold_int,
                        "total_revenue": total_revenue_float,
                        "image_urls": json.dumps(product.get('imageUrls', [])) if product.get('imageUrls') else '',
                        "created_at": product.get('createdAt', ''),
                        "updated_at": product.get('updatedAt', ''),
                        "has_details": has_details,
                        "stored_at": datetime.now().isoformat(),
                        "purpose": "analytics"
                    }
                    
                    # Add parsed details to metadata if available
                    if product.get('details'):
                        try:
                            import json
                            details = json.loads(product['details']) if isinstance(product['details'], str) else product['details']
                            if details:
                                # Store key details in metadata for easy filtering
                                if details.get('brand'):
                                    product_metadata['brand'] = details['brand']
                                if details.get('model'):
                                    product_metadata['model'] = details['model']
                                if details.get('color'):
                                    product_metadata['color'] = details['color']
                                if details.get('warranty'):
                                    product_metadata['warranty'] = details['warranty']
                                # Store full details as JSON string
                                product_metadata['details_json'] = json.dumps(details, ensure_ascii=False)
                        except:
                            pass
                    
                    # Validate and sanitize metadata for ChromaDB compatibility
                    sanitized_metadata = sanitize_metadata(product_metadata)
                    
                    # L∆∞u v√†o collection
                    business_collection.upsert(
                        documents=[product_content],
                        metadatas=[sanitized_metadata],
                        ids=[f"product_{product_id}"]
                    )
                    
                    sync_results["products"]["success"] += 1
                    print(f"[Sync] Stored product {product_id} with details: {has_details}")
                    
                except Exception as e:
                    sync_results["products"]["errors"] += 1
                    error_msg = f"Product {product.get('id', 'unknown')}: {str(e)}"
                    sync_results["errors"].append(error_msg)
                    print(f"[Sync] Error: {error_msg}")
                    import traceback
                    traceback.print_exc()
        
        # ƒê·ªìng b·ªô Orders
        if data.get('orders'):
            sync_results["orders"]["total"] = len(data['orders'])
            print(f"[Sync] Syncing {len(data['orders'])} orders...")
            
            for order in data['orders']:
                try:
                    order_id = str(order.get('id', ''))
                    
                    # T·∫°o n·ªôi dung order
                    order_content = f"""
Order ID: {order.get('id')}
Customer: {order.get('customerName', '')}
Status: {order.get('status', '')}
Total Amount: {order.get('totalAmount', 0)} VND
Items Count: {order.get('totalItems', 0)}
Created: {order.get('createdAt', '')}
"""
                    
                    # Safe conversion v·ªõi x·ª≠ l√Ω null/None
                    total_amount = order.get('totalAmount')
                    total_amount_float = float(total_amount) if total_amount is not None else 0.0
                    
                    total_items = order.get('totalItems')
                    total_items_int = int(total_items) if total_items is not None else 0
                    
                    # L∆∞u order items n·∫øu c√≥
                    order_items = order.get('items', [])
                    items_detail = []
                    for item in order_items:
                        items_detail.append({
                            "product_id": str(item.get('productId', '')),
                            "product_name": item.get('productName', ''),
                            "quantity": int(item.get('quantity', 0)) if item.get('quantity') is not None else 0,
                            "price": float(item.get('price', 0)) if item.get('price') is not None else 0.0,
                            "subtotal": float(item.get('subtotal', 0)) if item.get('subtotal') is not None else 0.0
                        })
                    
                    order_metadata = {
                        "data_type": "order",
                        "order_id": order_id,
                        "customer_name": order.get('customerName', ''),
                        "customer_id": str(order.get('customerId', '')),
                        "status": order.get('status', ''),
                        "total_amount": total_amount_float,
                        "total_items": total_items_int,
                        "created_at": order.get('createdAt', ''),
                        "updated_at": order.get('updatedAt', ''),
                        "payment_method": order.get('paymentMethod', ''),
                        "shipping_address": order.get('shippingAddress', ''),
                        "items_json": json.dumps(items_detail, ensure_ascii=False),  # Store items as JSON string
                        "stored_at": datetime.now().isoformat()
                    }
                    
                    # Validate and sanitize metadata for ChromaDB compatibility
                    sanitized_order_metadata = sanitize_metadata(order_metadata)
                    
                    # L∆∞u v√†o orders_analytics collection
                    orders_collection.upsert(
                        documents=[order_content],
                        metadatas=[sanitized_order_metadata],
                        ids=[f"order_{order_id}"]
                    )
                    
                    sync_results["orders"]["success"] += 1
                    
                except Exception as e:
                    sync_results["orders"]["errors"] += 1
                    error_msg = f"Order {order.get('id', 'unknown')}: {str(e)}"
                    sync_results["errors"].append(error_msg)
                    print(f"[Sync] Error: {error_msg}")
        
        # ƒê·ªìng b·ªô Categories
        if data.get('categories'):
            sync_results["categories"]["total"] = len(data['categories'])
            print(f"[Sync] Syncing {len(data['categories'])} categories...")
            
            for category in data['categories']:
                try:
                    category_id = str(category.get('id', ''))
                    
                    category_content = f"""
Category ID: {category.get('id')}
Name: {category.get('name', '')}
Description: {category.get('description', '')}
Status: {category.get('status', '')}
Product Count: {category.get('productCount', 0)}
"""
                    
                    # Safe conversion
                    product_count = category.get('productCount')
                    product_count_int = int(product_count) if product_count is not None else 0
                    
                    category_metadata = {
                        "data_type": "category",
                        "category_id": category_id,
                        "name": category.get('name', ''),
                        "description": category.get('description', ''),
                        "status": category.get('status', ''),
                        "product_count": product_count_int,
                        "created_at": category.get('createdAt', ''),
                        "updated_at": category.get('updatedAt', ''),
                        "image_url": category.get('imageUrl', ''),
                        "stored_at": datetime.now().isoformat()
                    }
                    
                    # Validate and sanitize metadata for ChromaDB compatibility
                    sanitized_category_metadata = sanitize_metadata(category_metadata)
                    
                    # Use business_collection directly
                    business_collection.upsert(
                        documents=[category_content],
                        metadatas=[sanitized_category_metadata],
                        ids=[f"category_{category_id}"]
                    )
                    
                    sync_results["categories"]["success"] += 1
                    
                except Exception as e:
                    sync_results["categories"]["errors"] += 1
                    error_msg = f"Category {category.get('id', 'unknown')}: {str(e)}"
                    sync_results["errors"].append(error_msg)
        
        # ƒê·ªìng b·ªô Business Performance
        if data.get('businessPerformance'):
            sync_results["business_performance"]["total"] = len(data['businessPerformance'])
            print(f"[Sync] Syncing {len(data['businessPerformance'])} business performance records...")
            
            for business in data['businessPerformance']:
                try:
                    business_id = str(business.get('businessId', ''))
                    
                    business_content = f"""
Business ID: {business.get('businessId')}
Username: {business.get('businessUsername', '')}
Total Products: {business.get('totalProducts', 0)}
Active Products: {business.get('activeProducts', 0)}
Total Orders: {business.get('totalOrders', 0)}
Revenue: {business.get('revenue', 0)} VND
Average Order Value: {business.get('averageOrderValue', 0)} VND
"""
                    
                    # Safe conversion cho business data
                    total_products = business.get('totalProducts')
                    total_products_int = int(total_products) if total_products is not None else 0
                    
                    active_products = business.get('activeProducts')
                    active_products_int = int(active_products) if active_products is not None else 0
                    
                    total_orders = business.get('totalOrders')
                    total_orders_int = int(total_orders) if total_orders is not None else 0
                    
                    revenue = business.get('revenue')
                    revenue_float = float(revenue) if revenue is not None else 0.0
                    
                    inventory_value = business.get('inventoryValue')
                    inventory_value_float = float(inventory_value) if inventory_value is not None else 0.0
                    
                    avg_order = business.get('averageOrderValue')
                    avg_order_float = float(avg_order) if avg_order is not None else 0.0
                    
                    business_metadata = {
                        "data_type": "business_performance",
                        "business_id": business_id,
                        "username": business.get('businessUsername', ''),
                        "total_products": total_products_int,
                        "active_products": active_products_int,
                        "inactive_products": safe_int(business.get('inactiveProducts')),
                        "total_orders": total_orders_int,
                        "completed_orders": safe_int(business.get('completedOrders')),
                        "revenue": revenue_float,
                        "inventory_value": inventory_value_float,
                        "average_order_value": avg_order_float,
                        "total_sold": safe_int(business.get('totalSold')),
                        "stored_at": datetime.now().isoformat()
                    }
                    
                    # Validate and sanitize metadata for ChromaDB compatibility
                    sanitized_business_metadata = sanitize_metadata(business_metadata)
                    
                    # Use business_collection directly
                    business_collection.upsert(
                        documents=[business_content],
                        metadatas=[sanitized_business_metadata],
                        ids=[f"business_{business_id}"]
                    )
                    
                    sync_results["business_performance"]["success"] += 1
                    
                except Exception as e:
                    sync_results["business_performance"]["errors"] += 1
                    error_msg = f"Business {business.get('businessId', 'unknown')}: {str(e)}"
                    sync_results["errors"].append(error_msg)
        
        # ƒê·ªìng b·ªô Discounts
        if data.get('discounts'):
            sync_results["discounts"]["total"] = len(data['discounts'])
            print(f"[Sync] Syncing {len(data['discounts'])} discounts...")
            
            for discount in data['discounts']:
                try:
                    discount_id = str(discount.get('id', ''))
                    
                    discount_content = f"""
Discount ID: {discount.get('id')}
Code: {discount.get('code', '')}
Type: {discount.get('discountType', '')}
Value: {discount.get('discountValue', 0)}
Status: {discount.get('status', '')}
Usage Count: {discount.get('usageCount', 0)}
"""
                    
                    # Safe conversion cho discount data
                    discount_value = discount.get('discountValue')
                    discount_value_float = float(discount_value) if discount_value is not None else 0.0
                    
                    min_order = discount.get('minOrderValue')
                    min_order_float = float(min_order) if min_order is not None else 0.0
                    
                    max_discount = discount.get('maxDiscountAmount')
                    max_discount_float = float(max_discount) if max_discount is not None else 0.0
                    
                    usage_limit = discount.get('usageLimit')
                    usage_limit_int = int(usage_limit) if usage_limit is not None else 0
                    
                    used_count = discount.get('usedCount')
                    used_count_int = int(used_count) if used_count is not None else 0
                    
                    # Parse additional fields
                    total_savings = discount.get('totalSavings')
                    total_savings_float = float(total_savings) if total_savings is not None else 0.0
                    
                    usage_percentage = discount.get('usagePercentage')
                    usage_percentage_float = float(usage_percentage) if usage_percentage is not None else 0.0
                    
                    discount_metadata = {
                        "data_type": "discount",
                        "discount_id": discount_id,
                        "code": discount.get('code', ''),
                        "name": discount.get('name', ''),
                        "description": discount.get('description', ''),
                        "type": discount.get('discountType', ''),
                        "value": discount_value_float,
                        "min_order_value": min_order_float,
                        "max_discount_amount": max_discount_float,
                        "usage_limit": usage_limit_int,
                        "used_count": used_count_int,
                        "status": discount.get('status', ''),
                        "start_date": discount.get('startDate', ''),
                        "end_date": discount.get('endDate', ''),
                        "created_at": discount.get('createdAt', ''),
                        "created_by_username": discount.get('createdByUsername', ''),
                        "created_by_id": str(discount.get('createdById', '')) if discount.get('createdById') else '',
                        "is_valid": discount.get('isValid', False),
                        "is_expired": discount.get('isExpired', False),
                        "usage_limit_reached": discount.get('usageLimitReached', False),
                        "usage_percentage": usage_percentage_float,
                        "total_savings": total_savings_float,
                        "stored_at": datetime.now().isoformat()
                    }
                    
                    # Validate and sanitize metadata for ChromaDB compatibility
                    sanitized_discount_metadata = sanitize_metadata(discount_metadata)
                    
                    # Use business_collection directly
                    business_collection.upsert(
                        documents=[discount_content],
                        metadatas=[sanitized_discount_metadata],
                        ids=[f"discount_{discount_id}"]
                    )
                    
                    sync_results["discounts"]["success"] += 1
                    
                except Exception as e:
                    sync_results["discounts"]["errors"] += 1
                    error_msg = f"Discount {discount.get('id', 'unknown')}: {str(e)}"
                    sync_results["errors"].append(error_msg)
        
        # ƒê·ªìng b·ªô Users (n·∫øu c√≥)
        if data.get('users'):
            sync_results["users"] = {"total": len(data['users']), "success": 0, "errors": 0}
            print(f"[Sync] Syncing {len(data['users'])} users...")
            
            for user in data['users']:
                try:
                    user_id = str(user.get('id', ''))
                    
                    user_content = f"""
User ID: {user.get('id')}
Username: {user.get('username', '')}
Email: {user.get('email', '')}
Role: {user.get('role', '')}
Status: {user.get('accountStatus', '')}
Phone: {user.get('phoneNumber', '')}
Address: {user.get('address', '')}
"""
                    
                    user_metadata = {
                        "data_type": "user",
                        "user_id": user_id,
                        "username": user.get('username', ''),
                        "email": user.get('email', ''),
                        "role": user.get('role', ''),
                        "account_status": user.get('accountStatus', ''),
                        "phone_number": user.get('phoneNumber', ''),
                        "address": user.get('address', ''),
                        "stored_at": datetime.now().isoformat()
                    }
                    
                    # Validate and sanitize metadata for ChromaDB compatibility
                    sanitized_user_metadata = sanitize_metadata(user_metadata)
                    
                    business_collection.upsert(
                        documents=[user_content],
                        metadatas=[sanitized_user_metadata],
                        ids=[f"user_{user_id}"]
                    )
                    
                    sync_results["users"]["success"] += 1
                    
                except Exception as e:
                    sync_results["users"]["errors"] += 1
                    error_msg = f"User {user.get('id', 'unknown')}: {str(e)}"
                    sync_results["errors"].append(error_msg)
        
        # ƒê·ªìng b·ªô Business Documents (n·∫øu c√≥) - L∆ØU V√ÄO COLLECTION RI√äNG BI·ªÜT
        if data.get('businessDocuments'):
            sync_results["documents"] = {"total": len(data['businessDocuments']), "success": 0, "errors": 0}
            print(f"[Sync] Syncing {len(data['businessDocuments'])} business documents...")
            
            # T·∫°o collection ri√™ng cho documents n·∫øu ch∆∞a c√≥
            try:
                documents_collection = chroma_client.get_or_create_collection(
                    name="business_documents",
                    metadata={"description": "Business documents for RAG analysis"}
                )
                print("[Sync] Documents collection ready")
            except Exception as e:
                print(f"[Sync] Error creating documents collection: {e}")
                sync_results["errors"].append(f"Documents collection error: {str(e)}")
                documents_collection = None
            
            for doc in data['businessDocuments']:
                try:
                    doc_id = str(doc.get('id', ''))
                    file_path = doc.get('filePath', '')
                    file_type = doc.get('fileType', '')
                    
                    # Resolve ƒë∆∞·ªùng d·∫´n file t·ª´ Spring Service
                    resolved_file_path = resolve_spring_file_path(file_path)
                    print(f"[Sync] Original path: {file_path} -> Resolved path: {resolved_file_path}")
                    
                    # Kh·ªüi t·∫°o document processor
                    doc_processor = get_document_processor()
                    
                    # Extract text content t·ª´ file
                    extracted_text = ""
                    processing_metadata = {}
                    
                    if resolved_file_path and os.path.exists(resolved_file_path):
                        try:
                            extracted_text, processing_metadata = doc_processor.extract_text_from_file(
                                resolved_file_path, file_type
                            )
                            print(f"[Sync] Successfully extracted {len(extracted_text)} characters from {doc.get('fileName', '')}")
                        except Exception as extract_error:
                            print(f"[Sync] Error extracting text from {resolved_file_path}: {extract_error}")
                            # Fallback: t·∫°o content t·ª´ metadata
                            extracted_text = f"Error extracting content from file: {str(extract_error)}"
                    else:
                        print(f"[Sync] File not found: {resolved_file_path} (original: {file_path})")
                        extracted_text = "File not found during sync process"
                    
                    # T·∫°o document content v·ªõi text ƒë√£ extract + metadata
                    file_size = doc.get('fileSize')
                    file_size_int = int(file_size) if file_size is not None else 0
                    
                    # K·∫øt h·ª£p extracted text v·ªõi metadata ƒë·ªÉ t·∫°o content ƒë·∫ßy ƒë·ªß
                    doc_content = f"""
DOCUMENT CONTENT:
{extracted_text}

---
METADATA:
Document ID: {doc.get('id')}
Business: {doc.get('businessUsername', '')}
File Name: {doc.get('fileName', '')}
File Type: {doc.get('fileType', '')}
Description: {doc.get('description', '')}
Size: {file_size_int} bytes
Uploaded: {doc.get('uploadedAt', '')}
Processing Status: {'Success' if processing_metadata.get('extraction_success') else 'Failed'}
Content Length: {len(extracted_text)} characters
"""
                    
                    doc_metadata = {
                        "data_type": "document",
                        "document_id": doc_id,
                        "business_id": str(doc.get('businessId', '')) if doc.get('businessId') else '',
                        "business_username": doc.get('businessUsername', ''),
                        "file_name": doc.get('fileName', ''),
                        "file_type": doc.get('fileType', ''),
                        "file_path_original": doc.get('filePath', ''),  # ƒê∆∞·ªùng d·∫´n g·ªëc t·ª´ Spring
                        "file_path_resolved": resolved_file_path or '',  # ƒê∆∞·ªùng d·∫´n ƒë√£ resolve
                        "file_size": file_size_int,
                        "description": doc.get('description', ''),
                        "uploaded_at": doc.get('uploadedAt', ''),
                        "stored_at": datetime.now().isoformat(),
                        "extraction_success": processing_metadata.get('extraction_success', False),
                        "content_length": len(extracted_text),
                        "processing_timestamp": processing_metadata.get('processing_timestamp', datetime.now().isoformat())
                    }
                    
                    # Th√™m metadata t·ª´ qu√° tr√¨nh processing n·∫øu c√≥
                    if "sheets" in processing_metadata:
                        doc_metadata["excel_sheets"] = json.dumps(processing_metadata["sheets"])
                    if "columns" in processing_metadata:
                        doc_metadata["csv_columns"] = processing_metadata["columns"]
                    if "rows" in processing_metadata:
                        doc_metadata["data_rows"] = processing_metadata["rows"]
                    
                    # Validate and sanitize metadata for ChromaDB compatibility
                    sanitized_doc_metadata = sanitize_metadata(doc_metadata)
                    
                    # L∆∞u v√†o collection ri√™ng bi·ªát cho documents
                    if documents_collection:
                        documents_collection.upsert(
                            documents=[doc_content],
                            metadatas=[sanitized_doc_metadata],
                            ids=[f"document_{doc_id}"]
                        )
                        print(f"[Sync] Stored document {doc_id} in separate collection")
                    else:
                        # Fallback: l∆∞u v√†o business_collection n·∫øu kh√¥ng t·∫°o ƒë∆∞·ª£c collection ri√™ng
                        business_collection.upsert(
                            documents=[doc_content],
                            metadatas=[sanitized_doc_metadata],
                            ids=[f"document_{doc_id}"]
                        )
                        print(f"[Sync] Fallback: Stored document {doc_id} in business collection")
                    
                    sync_results["documents"]["success"] += 1
                    
                except Exception as e:
                    sync_results["documents"]["errors"] += 1
                    error_msg = f"Document {doc.get('id', 'unknown')}: {str(e)}"
                    sync_results["errors"].append(error_msg)
        
        # Th√™m revenue overview t·ª´ data g·ªëc
        sync_results["revenue_overview"] = {
            "total_revenue": safe_decimal(data.get('totalRevenue')),
            "monthly_revenue": safe_decimal(data.get('monthlyRevenue')),
            "weekly_revenue": safe_decimal(data.get('weeklyRevenue')),
            "daily_revenue": safe_decimal(data.get('dailyRevenue')),
        }
        
        # L∆∞u revenue overview v√†o ChromaDB ƒë·ªÉ AI c√≥ th·ªÉ truy v·∫•n
        try:
            revenue_content = f"""
Revenue Overview - System Statistics
Total Revenue: {sync_results["revenue_overview"]["total_revenue"]} VND
Monthly Revenue: {sync_results["revenue_overview"]["monthly_revenue"]} VND
Weekly Revenue: {sync_results["revenue_overview"]["weekly_revenue"]} VND
Daily Revenue: {sync_results["revenue_overview"]["daily_revenue"]} VND
Last Updated: {datetime.now().isoformat()}
"""
            
            revenue_metadata = {
                "data_type": "revenue_overview",
                "total_revenue": sync_results["revenue_overview"]["total_revenue"],
                "monthly_revenue": sync_results["revenue_overview"]["monthly_revenue"],
                "weekly_revenue": sync_results["revenue_overview"]["weekly_revenue"],
                "daily_revenue": sync_results["revenue_overview"]["daily_revenue"],
                "stored_at": datetime.now().isoformat(),
                "purpose": "analytics"
            }
            
            # Validate and sanitize metadata
            sanitized_revenue_metadata = sanitize_metadata(revenue_metadata)
            
            revenue_collection.upsert(
                documents=[revenue_content],
                metadatas=[sanitized_revenue_metadata],
                ids=["revenue_overview_system"]
            )
            
            print("[Sync] Stored revenue overview in ChromaDB")
            
        except Exception as e:
            print(f"[Sync] Error storing revenue overview: {str(e)}")
            sync_results["errors"].append(f"Revenue overview storage error: {str(e)}")
        
        # Th√™m top selling products t·ª´ data g·ªëc
        if data.get('topSellingProducts'):
            sync_results["top_selling_products"] = [
                {
                    "product_id": str(p.get('productId', '')),
                    "product_name": p.get('productName', ''),
                    "total_sold": safe_int(p.get('totalSold')),
                    "revenue": safe_decimal(p.get('revenue'))
                }
                for p in data.get('topSellingProducts', [])[:10]
            ]
        
        # Th√™m low stock products t·ª´ data g·ªëc
        if data.get('lowStockProducts'):
            sync_results["low_stock_products"] = [
                {
                    "product_id": str(p.get('productId', '')),
                    "product_name": p.get('productName', ''),
                    "quantity": safe_int(p.get('quantity')),
                    "category": p.get('categoryName', '')
                }
                for p in data.get('lowStockProducts', [])
            ]
        
        # T·∫°o summary
        total_success = (
            sync_results["products"]["success"] +
            sync_results["orders"]["success"] +
            sync_results["categories"]["success"] +
            sync_results["business_performance"]["success"] +
            sync_results["discounts"]["success"] +
            sync_results.get("users", {}).get("success", 0) +
            sync_results.get("documents", {}).get("success", 0)
        )
        
        total_errors = (
            sync_results["products"]["errors"] +
            sync_results["orders"]["errors"] +
            sync_results["categories"]["errors"] +
            sync_results["business_performance"]["errors"] +
            sync_results["discounts"]["errors"] +
            sync_results.get("users", {}).get("errors", 0) +
            sync_results.get("documents", {}).get("errors", 0)
        )
        
        sync_results["summary"] = {
            "total_success": total_success,
            "total_errors": total_errors,
            "success_rate": f"{(total_success / (total_success + total_errors) * 100):.2f}%" if (total_success + total_errors) > 0 else "0%",
            "total_users": safe_int(data.get('totalUsers')),
            "total_customers": safe_int(data.get('totalCustomers')),
            "total_business_users": safe_int(data.get('totalBusinessUsers')),
            "total_products": safe_int(data.get('totalProducts')),
            "active_products": safe_int(data.get('activeProducts')),
            "total_orders": safe_int(data.get('totalOrders')),
            "delivered_orders": safe_int(data.get('deliveredOrders')),
            "pending_orders": safe_int(data.get('pendingOrders'))
        }
        
        print(f"[Sync] Completed: {total_success} success, {total_errors} errors")
        
        return sync_results
        
    except requests.RequestException as e:
        raise HTTPException(status_code=500, detail=f"Error connecting to Spring Service: {str(e)}")
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Sync error: {str(e)}")


@router.post("/clear-chroma")
async def clear_chroma_data():
    """
    Clear all data from ChromaDB collections
    """
    try:
        print("[ClearChroma] Starting ChromaDB data clearing process...")

        # Get ChromaDB client
        global chroma_client
        if chroma_client is None:
            raise HTTPException(status_code=500, detail="ChromaDB client not initialized")

        # Get all collections
        collections = chroma_client.list_collections()
        print(f"[ClearChroma] Found {len(collections)} collections to clear")

        cleared_collections = []
        errors = []

        for collection in collections:
            try:
                collection_name = collection.name
                print(f"[ClearChroma] Clearing collection: {collection_name}")

                # Delete the entire collection
                chroma_client.delete_collection(name=collection_name)

                cleared_collections.append(collection_name)
                print(f"[ClearChroma] Successfully cleared collection: {collection_name}")

            except Exception as e:
                error_msg = f"Error clearing collection {collection.name}: {str(e)}"
                print(f"[ClearChroma] {error_msg}")
                errors.append(error_msg)

        result = {
            "success": True,
            "cleared_collections": cleared_collections,
            "errors": errors,
            "total_cleared": len(cleared_collections),
            "total_errors": len(errors)
        }

        print(f"[ClearChroma] Clearing completed. Cleared: {len(cleared_collections)}, Errors: {len(errors)}")

        return result

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to clear ChromaDB data: {str(e)}")

